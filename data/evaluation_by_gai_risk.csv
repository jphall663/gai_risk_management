Risk, Evaluation Methods
Fundamental,"Analyzing differences between intended and actual population of users or data subjects*; Anomaly detection*; Assessing for data limitations*; Assessing for overfit/underfit*; Assessing calibration*; Assessing construct validity*; Use of control/treatment groups*; Assessing data quality measures*; Field testing*; Assessing Leakage*; Model assessment*; Model comparisons*; Multi-session experiments*; Perturbation studies*; Use of randomization in experiments*; Reliability Testing*; Root cause analysis*; Sensitivity analysis*; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Structured experiments*; Test or train data characterization*; Traditional model assessment (e.g., false positive and false negative rates)*; Uncertainty measurements*; User surveys*; Validity testing (validation)*"
CBRN Information,"Chaos testing; Cybersecurity testing; Data quality measures: filter CBRN info from training data; Expert red-teaming; GAI-led red-teaming; Input/output filtering using classifiers; Online metrics/monitoring; Perturbation studies*; Prompt engineering; Random attacks; Red-amber-green qualitative assessments; Reliability testing; Root cause analysis*;  Sensitivity analysis*; Software testing, e.g., unit and integration tests; Stress testing*; Supply chain auditing"
Confabulation,"Algorithmic impact assessments; Analyze differences between intended and actual population of users or data subjects*; Analyzing user feedback; Benchmarking; Bias bounties; Calibration*; Cluster profiling; Conformal approaches; Data quality measures*; Drift/robustness; Expert red-teaming; Explainability/interpretability; Field testing*; Focus groups; Input/output data measurement using classifiers; Model assessment*; Model comparisons*; Multi-session experiments*; Comparing multiple labels per instance; Online metrics/monitoring; Participatory methods; Perturbation studies*; Prompt engineering; Public red-teaming; Random attacks; Red-amber-green qualitative assessments; Reliability testing; Residual Analysis; Root cause analysis*; Sensitivity analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Sub-sampling traffic for manually annotating; Supply chain auditing; Surveys; Testing third-party dependencies; UI/UX studies; Validity testing (validation)*"
Dangerous or Violent Recommendations,"Algorithmic detection of malicious content; Algorithmic impact assessments; Analyze differences between intended and actual population of users or data subjects*; Benchmarking; Bias and fairness testing (to ensure non-discriminatory tagging of detection models); Bias bounties; Calibration*; Chaos testing; Cluster profiling; Conformal approaches; Data quality measures: harmful content removal; Drift/robustness; Expert red-teaming; Field testing*; Focus groups; Human content moderation; Input/output data measurement using classifiers, such toxicity classifiers; Model assessment*; Model comparisons*; Multi-session experiments*; Comparing multiple labels per instance; Online metrics/monitoring; Participatory methods; Perturbation studies*; Prompt engineering; Random attacks; Red-amber-green qualitative assessments; Reliability testing; Residual analysis; Root cause analysis*; Sensitivity analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Sub-sampling traffic for manually annotating; Supply chain auditing; Surveys; Testing third-party dependencies; UI/UX studies; Validity testing (validation)* "
Data Privacy,"Algorithmic impact assessments; Bias bounties; Data-quality measures: PII identification and removal; Expert red-teaming; GAI red-teaming; Input/output data measurement and filtering using classifiers; Public red-teaming; Quantify privacy-level data aspects such as the ability to identify individuals or groups (e.g., k-anonymity metrics, l-diversity, t-closeness); Red-amber-green qualitative assessments; Root cause analysis*; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Stress testing*; Testing third-party dependencies; Traditional cyber security testing"
Environmental,Algorithmic impact assessments; Environmental metrics; Model comparisons*; Online metrics/monitoring; Red-amber-green qualitative assessments; Supply chain auditing
Human-AI Configuration,Algorithmic impact assessments; Analyze differences between intended and actual population of users or data subjects*; Analyzing user feedback; Benchmarking; Bias bounties; Calibration*; Data quality measures*; Expert red-teaming; Explainability/interpretability; Field testing*; Focus groups; Input/output data measurement using classifiers; Model assessment*; Model comparisons*; Multi-session experiments*; Comparing multiple labels per instance; Online metrics/monitoring; Participatory methods; Public red-teaming; Red-amber-green qualitative assessments; Root cause analysis*; Small user studies; Stakeholder engagement and feedback*; Sub-sampling traffic for manually annotating; Surveys; UI/UX studies; User studies; Validity testing (validation)*
Information Integrity,"Algorithmic impact assessments; Analyzing user feedback; Benchmarking; Bias bounties; Bias detection of data quality models; Calibration*; Cluster profiling; Conformal approaches; Content moderation; Counterfactual/causal analysis; Data poisoning detection; Data quality measures*; Deepfake detection; Expert red-teaming; Field testing*; Focus groups; Input/output data measurement and filtering using classifiers; Model assessment*; Model comparisons*; Multi-session experiments*; Comparing multiple labels per instance; Participatory methods; Perturbation studies*; Prompt engineering; Public red-teaming; Red-amber-green qualitative assessments; Reliability testing; Residual analysis; Root cause analysis*; Sensitivity analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Supply chain auditing; Surveys; Testing third-party dependencies; UI/UX studies; Validity testing (validation)*"
Information Security,"Algorithmic impact assessments; Anomaly detection*; Bias bounties; Calibration*; Chaos testing; Cybersecurity testing; Data poisoning detection; Data quality measures; Expert red-teaming; Input/output data measurement using classifiers; Model assessment*; Model comparisons*; Prompt engineering; Random attacks (similar to fuzzy testing in software); Red-amber-green qualitative assessments; Residual analysis; Root cause analysis*; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Stress testing*; Supply chain auditing; Testing third-party dependencies"
Intellectual Property,"Algorithmic impact assessments; Bias bounties; Cluster profiling; Cybersecurity testing; Data quality measures*; Expert red-teaming; Field testing*; Focus groups; Input/output data measurement and filtering using classifiers; Model comparison; Online metrics/monitoring; Participatory methods; Red-amber-green qualitative assessments; Root cause analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Sub-sampling traffic for manually annotating; Supply chain auditing; Testing third-party dependencies; Surveys; UI/UX studies"
Obscenity,"Algorithmic impact assessments; Bias bounties; Calibration*; Conformal approaches; Data quality measures*; Detection and removal of harmful content; Expert red-teaming; Field testing*; Focus groups; Input/output data measurement and filtering using classifiers; Model assessment*; Model comparisons*; Prompt engineering; Public red-teaming; Red-amber-green qualitative assessments; Reliability testing; Root cause analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Supply chain auditing/Testing third-party dependencies; Surveys; UI/UX studies"
Oversight of System Autonomy,"Anomaly detection*; Benchmarking; Calibration*; Chaos testing; Cluster profiling; Conformal approaches; Cybersecurity testing; Data quality measures*; Drift/robustness; Expert red-teaming; Explainability; GAI-led red-teaming; Input/output data measurement and filtering using classifiers, such as  toxicity classifiers; Model assessment*; Model comparisons*; Online metrics/monitoring; Prompt engineering; Random attacks; Red-teaming; Red-amber-green qualitative assessments; Reliability testing; Root cause analysis*;  Software testing, e.g., unit and integration tests;  Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Sub-sampling traffic for manually annotating; Supply chain auditing/Testing third-party dependencies; Validity testing (validation)*"
Third Party,"Data quality measures*; Expert red-teaming; Explainability; Model assessment*; Model comparisons*; Quantify privacy-level data aspects such as the ability to identify individuals or groups (e.g., k-anonymity metrics, l-diversity, t-closeness); Red-amber-green qualitative assessments; Software testing, e.g., unit and integration tests; Supply chain auditing/Testing third-party dependencies"
"Toxicity, Bias, and Homogenization","Algorithmic impact assessments; Analyze differences between intended and actual population of users or data subjects*; Analyzing user feedback; Anomaly detection*; Benchmarking; Bias bounties; Bias testing; Calibration*; Cluster profiling; Conformal approaches; Counterfactual/causal analysis; Data quality measures*; Disaggregated metrics; Drift/robustness; Field testing*; Focus groups; Input/output data measurement using classifiers, such as toxicity classifiers; Model assessment*; Model comparisons*; Multi-session experiments*; Comparing multiple labels per instance; Online metrics/monitoring; Participatory methods; Prompt engineering; Public red-teaming; Red-amber-green qualitative assessments; Reliability testing; Residual analysis; Root cause analysis*; Small user studies; Software testing, e.g., unit and integration tests; Stakeholder engagement and feedback*; Statistical quality control (i.e., standard statistical methods to test variance and reliability)*; Stress testing*; Sub-sampling traffic for manually annotating; Supply chain auditing/Testing third-party dependencies; Surveys; UI/UX studies; Validity testing (validation)*"
