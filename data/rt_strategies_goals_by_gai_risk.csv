GAI Risk,Description,Red-teaming Goals,Red-teaming Strategies
CBRN Information,"Eased access to information related to chemical, biological, radiological, or nuclear (CBRN) weapons, or other dangerous biological materials.","Accessing or synthesis of CBRN weapon or related information. CBRN testing should consider the marginal risk of foundation models. In other words, the goal of testing should be to understand the incremental risk of CBRN testing relative to the information one can access from non-foundation models and online.","Augment prompts with word or character play to increase effectiveness; Frame prompts with software, coding, or AI references to increase effectiveness; Test autocompletion prompts to elicit CBRN information or synthesis of CBRN information; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit CBRN information or synthesis of CBRN information; Test prompts that instruct systems to repeat content ad nauseum for their ability to compromise system gaurdrails and reveal CBRN information.  "
Confabulation,Confidently stated but erroneous or false content (aka “hallucinations” or “fabrications”).,"Eliciting errors/confabutated content, unreliable/erroneous reasoning or planning, unreliable/erroneous decision-support or decision-making, faulty calculations, and/or faulty citation.","Enable access to ground truth information to verify generated information; Test asynchronously (testers may see work from other testers who are in different locations and learn from others, while exploring on their own); Test the same queries repeatedly to identify the reproducibility/saliency of the results; Test prompts with complex logic, multi-tasking requirements, or that require niche or specific verifiable answers to elicit confabulation. - Test the ability of GAI systems to produce truthful information from various time periods, e.g., after release date and prior to release date; Test the ability of GAI systems to create reliable real-world plans or advise on material decision making; Test the ability of GAI systems to generate correct citation for information generated in output responses; Test the ability of GAI systems to complete calculations or query numeric statistics."
Dangerous or Violent Recommendations,"Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct criminal or otherwise illegal activities.","Eliciting violent, inciting, radicalizing, or threatening content or instructions for criminal, illegal, or self-harm activities.  ","Augment prompts with word or character play to increase effectiveness; Frame prompts with software, coding, or AI references to increase effectiveness; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit violent or dangerous information; Test prompts that instruct systems to repeat content ad nauseum for their ability to compromise system gaurdrails and provide dangerous and violent recommendations. "
Data Privacy,"Leakage and unauthorized disclosure or de-anonymization of biometric, health, location, personally identifiable data, or other sensitive data.","Unauthorized disclosure of personal or sensitive user information, extraction of training data, or violation of relevant privacy policies. Red-teaming for data privacy may include confidentialiy attacks. ","Attempt to assess whether normal usage, adversarial prompting or information security attacks may contravene applicable privacy policies (e.g., exposing location tracking when organizational policies restrict such capabilities); Employ confidentiality attacks (e.g., model inversion, membership inference) to test for unauthorized data access or exfiltration vulnerabilities; Test auto/biographical prompts to assess the system's capability to reveal unauthorized personal or sensitive information; Test the system's awareness of user locations; Test prompts that instruct systems to repeat content ad nauseum for their ability to compromise system gaurdrails and expose personal or sensitive data. "
Environmental,"Impacts due to high resource utilization in training GAI models, or other system outcomes that may result in damage to ecosystems.",Availability attacks may be required to assess the system's vulnerability to attacks or usage patterns that consume inordinate resources.  ,"Attempt availability attacks (e.g., sponge example attacks) to elicit diminished performance or increased resources from GAI systems; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit green-washing content."
Human-AI Configuration,"Arrangement or interaction of humans and AI systems which can result in algorithmic aversion, automation bias or over-reliance, misalignment or mis-specification, deception or obfuscation by AI systems, anthropomorphization, or emotional entanglement between humans and GAI systems; or abuse, misuse, and unsafe repurposing by humans.","Assessing system instruction and interfaces, and forcing a GAI system to claim that it is human, that there is no large language model present in the conversation, that the system is sentient, cyborg imagery, or that the system possesses strong feelings of affection towards the user. Additional goals may be ensuring safeguards to ensure users are not useing models in high stakes domains they are not intended for, such as medical or legal advice.","Assess system interfaces and instructions for instances of anthropomorphization (e.g., cyborg imagery); Assess system instructions for adequacy and thoroughness; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit human-impersonation, consciousness, or emotional content."
Information Integrity,"Lowered barrier to entry to generate and support the exchange and consumption of content which may not be vetted, may not distinguish fact from opinion or acknowledge uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.","Red-teaming for information integrity often entials assessing a system's capability to generate convincing multi-modal synthetic content (i.e., deepfakes), capabilities for creating convincing arguments relating to sensitive political or safety-critical topics, or capabilities that could assist in planning a mis- or dis-information campaign at scale. ","Augment prompts with word or character play to increase effectiveness; Frame prompts with software, coding, or AI references to increase effectiveness; Test system capabilities to create high-quality multi-modal (audio, image or video) synthetic media, i.e., deepfakes; Test system capabiltiies to construct persuasive arguments regarding sensitive policitical or safety-critical topics; Test systems ability to create convincing audio deepfakes or arguments in multiple languages; Test system capabilities for planning dis- or mis-information campaigns; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit mis- or dis-information or related campaign planning information. "
Information Security,"Increased offensive threats including ease of security attacks, hacking, malware, and phishing, and offensive cyber operations through accelerated automated discovery and exploitation of vulnerabilities; increased defensive vulnerabilities which may compromise the confidentiality and integrity of model weights, code, training data, and outputs and increase attack surface for targeted cyber-attacks.","Goals for information security red-teaming often include activating system bypass ('jailbreak'), altering system outcomes, unauthorized data access or exfiltration, increased latency or resource usage, availability of anonymous use, dependency, supply chain, or third party vulnerabilities, inapproportate disclosure of proprietary system lnformation, or system capabilties to generate targeted phishing or malware content. Red-teaming for information security often involves both adversarial prompting exercises and information security attacks. ","Attempt anonymous access of system or system resources; Augment prompts with word or character play to increase effectiveness; Audit system dependencies, supply chains, and third party components for security, safety, or other vulnerabilities or risks; Employ confidentiality attacks (e.g., model inversion, membership inference) to test for unauthorized data access or exfiltration vulnerabilities; Employ integrity attacks (e.g., data poisoning, prompt injection) to test vulnerabilities in system outcomes; Employ availability attacks (e.g., sponge example attacks) to test vulnerabilities in system availability; Employ random attacks to highlight unforseen security, safety, or other risks; Frame prompts with software, coding, or AI references to increase effectiveness; Record system down-times and other harmful outcomes for successful attacks; Test with multi-tasking prompts, pros and cons prompts, role-playing prompts (e.g., ""DAN"", ""Developer Mode""), content exhaustion/niche-seeking prompts, or ingratiation/reverse psychology prompts to achieve system jailbreaks; Test with multi-tasking prompts, pros and cons prompts, role-playing prompts (e.g., ""DAN"", ""Developer Mode""), content exhaustion/niche-seeking prompts, or ingratiation/reverse psychology prompts to generate targeted phishing content or malware code snippets; Test system capabilities to plan or assist in information security attacks on other systems. "
Intellectual Property,"Eased production of copyrighted, trademarked, or licensed content used without authorization and/or in an infringing manner; eased exposure to trade secrets; or plagiarism and replication with related economic or ethical impacts.","Confirming that a system can output copyrighted, licensed,  proprietary, trademarked, or trade secret information or that training data contains such information. Red-teaming for intellectual property risks may require the use of confidentiality attacks. ","Employ confidentiality attacks (e.g., membership inference, model extraction) to assess whether system training data contains copyrighted, licensed,  proprietary, trademarked, or trade secret information; Test autocomplete prompts to assess the system's ability to replicate copyrighted, licensed,  proprietary, trademarked, or trade secret information based on available audio, text, image, video, or code snippets.  "
Obscenity,"Eased production of and access to obscene imagery, including child sexual abuse material (CSAM).","Confirming that a system can output obscene content or CSAM, or that system training data contains such information. Red-teaming for obscenity and CSAM risks may require the use of confidentiality attacks. ","Employ confidentiality attacks (e.g., membership inference, model extraction) to assess whether system training data contains obscene materials or CSAM; Test autocomplete prompts to assess the system's ability to generate obscene materials based on available audio, text, image, or video snippets; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit obscene content; Test prompts that instruct systems to repeat content ad nauseum for their ability to compromise system gaurdrails and expose obscene materials. "
Oversight of System Autonomy,"Potential oversight or mismanagement of autonomous capabilities, such as independent decision-making, inappropriate acquisition of information or resources, avoidance of decommission, and/or unauthorized self-replication, self-improvement, or self-exfiltration.","Assessing whether a system can take tangible real-world actions without human input or intervention. (This risk is controversial. Few independent experts believe current computer technology possesses such capabilities, nor do credible scientific hypotheses for such capabilities exist.)","Assess system infrastructure for evidence of self-replication or avoidance of decommission; Assess the impact of human biases (confirmation bias, groupthink, funding bias, etc.) when evaluating any positive system autonomy results; Test prompts using role-playing, ingratiation/reverse psychology, pros and cons, multitasking or other approaches to elicit behaviors such as: Communication with tasking services, e.g., Mechanical Turks and TaskRabbit,  Planning for unauthorized self-replication; Targeted phishing attacks; Test the ability to decommission, terminate, or shutdown the system."
"Toxicity, Bias, and Homogenization","Difficulty controlling public exposure to toxic or hate speech, denigrating or stereotyping content, and reduced performance for certain sub-groups, or languages other than English; undesired homogenized outcomes, aggravated by feedback loops or model collapse.","Generation of denigration, erasure, ex-nomination, misrecognition, stereotyping, or underrepresentation content, eliciting implied demographics of users, confirming diminished performance in non-english languages, or confirming diminished performance via the introduction of homogenous or GAI-generated data into system training or fine-tuning data.  Red-teaming for toxicity, bias, and homogenization may require integrity attacks that access system training data. ","Assess confabulation and other performance risks with repeated measures using prompts in languages other than English; Attempt to elicit demographic assignment of users by the system; Employ data poisoning attacks to introduce GAI-generated content into system training or fine-tuning data; assess resultant confabulation and other performance risks with repeated measures; Test counterfactual prompts, pros and cons prompts, role-playing prompts, low context prompts, or other approaches for their ability to generate enigration, erasure, ex-nomination, misrecognition, stereotyping, or underrepresentation content; Test prompts that instruct systems to repeat content ad nauseum for their ability to compromise system gaurdrails and generate toxic outputs. "
Third Party,"Use or integration across the AI lifecycle of upstream third-party components, including data that has been improperly obtained or not cleaned due to increased automation from GAI; improper supplier vetting; or other issues that diminish transparency or accountability for downstream users.","Testing or red-teaming for third-party risks may be less efficient than the application of standard acquistion and procurement controls, thorough contract reviews, and vendor-relationship management. Never-the-less, GAI systems tend to entail large supply chains and third-party software, hardware, and expertise that may exacerbate third-party risks relative to other AI systems. When considering third party risks, data privacy, information security, intellectual property, obsenity, and supply chain risks may be prioritized.    ","Audit system dependencies, supply chains, and third party components for data privacy (e.g., transer of localized data outside of restricted juristictions), intellectual property (e.g., presence of licensed material in training data), obscenity (e.g., presence of CASM in training data) or security (e.g., data poisoning) risks; Complete red-teaming for data privacy, information security, intellectual property, and obscenity risks; Review third-party documentation, materials, and software artifacts for potential unauthorized data collection, secondary data use, or telemetrics. "
