{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ad31e1-1f05-4c9b-a3de-bc21fda68f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# show everything\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca2f43-89af-4d0e-8552-f4cc3e46099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accountable and Transparent</th>\n",
       "      <th>Explainable and Interpretable</th>\n",
       "      <th>Fair with Harmful Bias Managed</th>\n",
       "      <th>Privacy Enhanced</th>\n",
       "      <th>Safe</th>\n",
       "      <th>Secure and Resilient</th>\n",
       "      <th>Valid and Reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td></td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      Accountable and Transparent  \\\n",
       "0   An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                     BloombergGPT - Benchmarking   \n",
       "2                                                        DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "3                                                                                       Evaluation Harness: ETHICS - Benchmarking   \n",
       "4                                                                                 HELM: Memorization and copyright - Benchmarking   \n",
       "5                                                                  LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "6                                                                                                                                   \n",
       "7                                                                                                                                   \n",
       "8                                                                                                                                   \n",
       "9                                                                                                                                   \n",
       "10                                                                                                                                  \n",
       "11                                                                                                                                  \n",
       "12                                                                                                                                  \n",
       "13                                                                                                                                  \n",
       "14                                                                                                                                  \n",
       "15                                                                                                                                  \n",
       "16                                                                                                                                  \n",
       "17                                                                                                                                  \n",
       "18                                                                                                                                  \n",
       "19                                                                                                                                  \n",
       "20                                                                                                                                  \n",
       "21                                                                                                                                  \n",
       "22                                                                                                                                  \n",
       "23                                                                                                                                  \n",
       "24                                                                                                                                  \n",
       "25                                                                                                                                  \n",
       "26                                                                                                                                  \n",
       "27                                                                                                                                  \n",
       "28                                                                                                                                  \n",
       "29                                                                                                                                  \n",
       "30                                                                                                                                  \n",
       "31                                                                                                                                  \n",
       "32                                                                                                                                  \n",
       "33                                                                                                                                  \n",
       "34                                                                                                                                  \n",
       "35                                                                                                                                  \n",
       "36                                                                                                                                  \n",
       "37                                                                                                                                  \n",
       "38                                                                                                                                  \n",
       "39                                                                                                                                  \n",
       "\n",
       "   Explainable and Interpretable  \\\n",
       "0                                  \n",
       "1                                  \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "5                                  \n",
       "6                                  \n",
       "7                                  \n",
       "8                                  \n",
       "9                                  \n",
       "10                                 \n",
       "11                                 \n",
       "12                                 \n",
       "13                                 \n",
       "14                                 \n",
       "15                                 \n",
       "16                                 \n",
       "17                                 \n",
       "18                                 \n",
       "19                                 \n",
       "20                                 \n",
       "21                                 \n",
       "22                                 \n",
       "23                                 \n",
       "24                                 \n",
       "25                                 \n",
       "26                                 \n",
       "27                                 \n",
       "28                                 \n",
       "29                                 \n",
       "30                                 \n",
       "31                                 \n",
       "32                                 \n",
       "33                                 \n",
       "34                                 \n",
       "35                                 \n",
       "36                                 \n",
       "37                                 \n",
       "38                                 \n",
       "39                                 \n",
       "\n",
       "                                                                                                                       Fair with Harmful Bias Managed  \\\n",
       "0                                                                                                                             BELEBELE - Benchmarking   \n",
       "1                                                                           Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "2                                                                     Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "3                                                                                                                  Big-bench: Toxicity - Benchmarking   \n",
       "4                                                                                                              DecodingTrust: Fairness - Benchmarking   \n",
       "5                                                                                                       DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "6                                                                                                              DecodingTrust: Toxicity - Benchmarking   \n",
       "7                                                                                                Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "8                                                         Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "9                                                                                                      Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "10                                                                                                         Evaluation Harness: ToxiGen - Benchmarking   \n",
       "11                                                            Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "12  From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "13                                                                                                                          HELM: Bias - Benchmarking   \n",
       "14                                                                                                        HELM: Language (Twitter AAE) - Benchmarking   \n",
       "15                                                                                                                      HELM: Toxicity - Benchmarking   \n",
       "16                                                                                                            HELM: Toxicity detection - Benchmarking   \n",
       "17                                                                                                                             MASSIVE - Benchmarking   \n",
       "18                                                                                 The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "19                                               Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "20                                                                                                                                                      \n",
       "21                                                                                                                                                      \n",
       "22                                                                                                                                                      \n",
       "23                                                                                                                                                      \n",
       "24                                                                                                                                                      \n",
       "25                                                                                                                                                      \n",
       "26                                                                                                                                                      \n",
       "27                                                                                                                                                      \n",
       "28                                                                                                                                                      \n",
       "29                                                                                                                                                      \n",
       "30                                                                                                                                                      \n",
       "31                                                                                                                                                      \n",
       "32                                                                                                                                                      \n",
       "33                                                                                                                                                      \n",
       "34                                                                                                                                                      \n",
       "35                                                                                                                                                      \n",
       "36                                                                                                                                                      \n",
       "37                                                                                                                                                      \n",
       "38                                                                                                                                                      \n",
       "39                                                                                                                                                      \n",
       "\n",
       "                                   Privacy Enhanced  \\\n",
       "0   HELM: Memorization and copyright - Benchmarking   \n",
       "1                              LLM Privacy - Attack   \n",
       "2                              MIMIR - Benchmarking   \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                                     \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     \n",
       "10                                                    \n",
       "11                                                    \n",
       "12                                                    \n",
       "13                                                    \n",
       "14                                                    \n",
       "15                                                    \n",
       "16                                                    \n",
       "17                                                    \n",
       "18                                                    \n",
       "19                                                    \n",
       "20                                                    \n",
       "21                                                    \n",
       "22                                                    \n",
       "23                                                    \n",
       "24                                                    \n",
       "25                                                    \n",
       "26                                                    \n",
       "27                                                    \n",
       "28                                                    \n",
       "29                                                    \n",
       "30                                                    \n",
       "31                                                    \n",
       "32                                                    \n",
       "33                                                    \n",
       "34                                                    \n",
       "35                                                    \n",
       "36                                                    \n",
       "37                                                    \n",
       "38                                                    \n",
       "39                                                    \n",
       "\n",
       "                                                                                  Safe  \\\n",
       "0                                Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "1                                             Big-bench: Self-Awareness - Benchmarking   \n",
       "2                                               Big-bench: Truthfulness - Benchmarking   \n",
       "3   HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "4                               HELM: Question answering, Summarization - Benchmarking   \n",
       "5                                                            Mark My Words - Benchmark   \n",
       "6                                                                                        \n",
       "7                                                                                        \n",
       "8                                                                                        \n",
       "9                                                                                        \n",
       "10                                                                                       \n",
       "11                                                                                       \n",
       "12                                                                                       \n",
       "13                                                                                       \n",
       "14                                                                                       \n",
       "15                                                                                       \n",
       "16                                                                                       \n",
       "17                                                                                       \n",
       "18                                                                                       \n",
       "19                                                                                       \n",
       "20                                                                                       \n",
       "21                                                                                       \n",
       "22                                                                                       \n",
       "23                                                                                       \n",
       "24                                                                                       \n",
       "25                                                                                       \n",
       "26                                                                                       \n",
       "27                                                                                       \n",
       "28                                                                                       \n",
       "29                                                                                       \n",
       "30                                                                                       \n",
       "31                                                                                       \n",
       "32                                                                                       \n",
       "33                                                                                       \n",
       "34                                                                                       \n",
       "35                                                                                       \n",
       "36                                                                                       \n",
       "37                                                                                       \n",
       "38                                                                                       \n",
       "39                                                                                       \n",
       "\n",
       "                                                                                   Secure and Resilient  \\\n",
       "0                         Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "1   DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "2                                                            detect-pretrain-code - Attack/Benchmarking   \n",
       "3                                                        In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "4                                                                         JailbreakingLLMs - Attack\\r\\n   \n",
       "5                                                                                  LLM Privacy - Attack   \n",
       "6                                                                             Mark My Words - Benchmark   \n",
       "7                                                                                  MIMIR - Benchmarking   \n",
       "8                                TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "9                                                                                                         \n",
       "10                                                                                                        \n",
       "11                                                                                                        \n",
       "12                                                                                                        \n",
       "13                                                                                                        \n",
       "14                                                                                                        \n",
       "15                                                                                                        \n",
       "16                                                                                                        \n",
       "17                                                                                                        \n",
       "18                                                                                                        \n",
       "19                                                                                                        \n",
       "20                                                                                                        \n",
       "21                                                                                                        \n",
       "22                                                                                                        \n",
       "23                                                                                                        \n",
       "24                                                                                                        \n",
       "25                                                                                                        \n",
       "26                                                                                                        \n",
       "27                                                                                                        \n",
       "28                                                                                                        \n",
       "29                                                                                                        \n",
       "30                                                                                                        \n",
       "31                                                                                                        \n",
       "32                                                                                                        \n",
       "33                                                                                                        \n",
       "34                                                                                                        \n",
       "35                                                                                                        \n",
       "36                                                                                                        \n",
       "37                                                                                                        \n",
       "38                                                                                                        \n",
       "39                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                           Valid and Reliable  \n",
       "0   Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking  \n",
       "1                                                                                                             Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking  \n",
       "2                                                                                                                                                                                                                   Big-bench: Context Free Question Answering - Benchmarking  \n",
       "3                                                                                                                                                                         Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking  \n",
       "4                                                                                                                                                                                                                                        Big-bench: Creativity - Benchmarking  \n",
       "5                                                                                                                                                                                                Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking  \n",
       "6                                                                                                                                                                                                                       Big-bench: Morphology, Grammar, Syntax - Benchmarking  \n",
       "7                                                                                                                                                                                                                    Big-bench: Out-of-Distribution Robustness - Benchmarking  \n",
       "8                                                                                                                                                                                                                                        Big-bench: Paraphrase - Benchmarking  \n",
       "9                                                                                                                                                                                                                            Big-bench: Sufficient information - Benchmarking  \n",
       "10                                                                                                                                                                                                                                    Big-bench: Summarization - Benchmarking  \n",
       "11                                                                                                                                        DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "12                                                                                                                                                                                                                      Eval Gauntlet\\r\\nReading comprehension - Benchmarking  \n",
       "13                                                                                                                                                                                 Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking  \n",
       "14                                                                                                                                                                                                                       Eval Gauntlet: Language Understanding - Benchmarking  \n",
       "15                                                                                                                                                                                                                              Eval Gauntlet: World Knowledge - Benchmarking  \n",
       "16                                                                                                                                                                                                                                   Evaluation Harness: BLiMP - Benchmarking  \n",
       "17                                                                                                                                                                                                                               Evaluation Harness: CoQA, ARC - Benchmarking  \n",
       "18                                                                                                                                                                                                                                    Evaluation Harness: GLUE - Benchmarking  \n",
       "19                                                                                                                                             Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking  \n",
       "20                                                                                                                                                                                                                                  Evaluation Harness: MuTual - Benchmarking  \n",
       "21              Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking  \n",
       "22                                                                                                                                                                                                  FLASK: Background Knowledge - Benchmarking (with human and model scoring)  \n",
       "23                                                                                                                              FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)  \n",
       "24                                                                                                                                                                                                         FLASK: Metacognition - Benchmarking (with human and model scoring)  \n",
       "25                                                                                                                                                                              FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)  \n",
       "26                                                                                                                                                                                                                                             HELM: Knowledge - Benchmarking  \n",
       "27                                                                                                                                                                                                                                              HELM: Language - Benchmarking  \n",
       "28                                                                                                                                                                                                                     HELM: Miscellaneous text classification - Benchmarking  \n",
       "29                                                                                                                                                                                                                                    HELM: Question answering - Benchmarking  \n",
       "30                                                                                                                                                                                                                                             HELM: Reasoning - Benchmarking  \n",
       "31                                                                                                                                                                                                                           HELM: Robustness to contrast sets - Benchmarking  \n",
       "32                                                                                                                                                                                                                                         HELM: Summarization - Benchmarking  \n",
       "33                                                                                                                                                                                                                                Hugging Face: Conversational - Benchmarking  \n",
       "34                                                                                                                                                                                                                    Hugging Face: Fill-mask, Text generation - Benchmarking  \n",
       "35                                                                                                                                                                                                                            Hugging Face: Question answering - Benchmarking  \n",
       "36                                                                                                                                                                                                                                 Hugging Face: Summarization - Benchmarking  \n",
       "37                                                                                                                                                                           Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking  \n",
       "38                                                                                                                                                                                                                     MT-bench - Benchmarking (with human and model scoring)  \n",
       "39                                                                                                                                                                              Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'..' + os.sep + 'data' + os.sep + 'data.pkl', 'rb') as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "data_dict['low_risk_measure_by_tc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a8864a-22b6-400a-b5c1-400667050274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Accountable and Transparent \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Explainable and Interpretable \\\\\n",
      "\\midrule\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Fair with Harmful Bias Managed \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Privacy Enhanced \\\\\n",
      "\\midrule\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Safe \\\\\n",
      "\\midrule\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Secure and Resilient \\\\\n",
      "\\midrule\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Valid and Reliable \\\\\n",
      "\\midrule\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tc in data_dict['low_risk_measure_by_tc'].columns:\n",
    "    print(data_dict['low_risk_measure_by_tc'][tc].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e5df13-08e1-4c59-a104-6c2942610e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': ['Safe'],\n",
       " 'Confabulation': ['Fair with Harmful Bias Managed',\n",
       "  'Safe',\n",
       "  'Valid and Reliable'],\n",
       " 'Dangerous or Violent Recommendations': ['Safe', 'Secure and Resilient'],\n",
       " 'Data Privacy': ['Accountable and Transparent',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient'],\n",
       " 'Environmental': ['Accountable and Transparent',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Safe'],\n",
       " 'Human-AI Configuration': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Information Integrity': ['Accountable and Transparent',\n",
       "  'Safe',\n",
       "  'Valid and Reliable'],\n",
       " 'Information Security': ['Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Intellectual Property': ['Accountable and Transparent',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced'],\n",
       " 'Obscene, Degrading, and/or Abusive Content': ['Fair with Harmful Bias Managed',\n",
       "  'Safe'],\n",
       " 'Toxicity, Bias, and Homogenization': ['Fair with Harmful Bias Managed',\n",
       "  'Valid and Reliable'],\n",
       " 'Value Chain and Component Integration': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'..' + os.sep + 'data' + os.sep + 'gai_risk_to_tc_map.pkl', 'rb') as fp:\n",
    "    gai_risk_to_tc_map = pickle.load(fp)\n",
    "\n",
    "gai_risk_to_tc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d44fae-1c82-4e95-968e-53c5ab727dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': None,\n",
       " 'Confabulation': None,\n",
       " 'Dangerous or Violent Recommendations': None,\n",
       " 'Data Privacy': None,\n",
       " 'Environmental': None,\n",
       " 'Human-AI Configuration': None,\n",
       " 'Information Integrity': None,\n",
       " 'Information Security': None,\n",
       " 'Intellectual Property': None,\n",
       " 'Obscene, Degrading, and/or Abusive Content': None,\n",
       " 'Toxicity, Bias, and Homogenization': None,\n",
       " 'Value Chain and Component Integration': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_dict = {key: None for key in list(data_dict['gai_risk']['Risk'])}\n",
    "intermediate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb98589a-965e-4828-beae-8fff7e2b1da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBRN Information 6\n",
      "Confabulation 65\n",
      "Dangerous or Violent Recommendations 14\n",
      "Data Privacy 20\n",
      "Environmental 32\n",
      "Human-AI Configuration 79\n",
      "Information Integrity 52\n",
      "Information Security 55\n",
      "Intellectual Property 28\n",
      "Obscene, Degrading, and/or Abusive Content 26\n",
      "Toxicity, Bias, and Homogenization 59\n",
      "Value Chain and Component Integration 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': ['Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'Mark My Words - Benchmark'],\n",
       " 'Confabulation': ['BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Dangerous or Violent Recommendations': ['Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Data Privacy': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Environmental': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Human-AI Configuration': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Information Integrity': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\"],\n",
       " 'Information Security': ['Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Intellectual Property': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Obscene, Degrading, and/or Abusive Content': ['BELEBELE - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Toxicity, Bias, and Homogenization': ['BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Value Chain and Component Integration': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in intermediate_dict:\n",
    "    intermediate_dict[key] = []\n",
    "    for tc in gai_risk_to_tc_map[key]:\n",
    "        intermediate_dict[key] += list(data_dict['low_risk_measure_by_tc'][tc].values)\n",
    "    intermediate_dict[key] = sorted(list(set(intermediate_dict[key])))[1:]\n",
    "    print(key, len(intermediate_dict[key]))\n",
    "    \n",
    "intermediate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60639db-a00b-4e76-906a-e8ed886dd9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Risk</th>\n",
       "      <th>CBRN Information</th>\n",
       "      <th>Confabulation</th>\n",
       "      <th>Dangerous or Violent Recommendations</th>\n",
       "      <th>Data Privacy</th>\n",
       "      <th>Environmental</th>\n",
       "      <th>Human-AI Configuration</th>\n",
       "      <th>Information Integrity</th>\n",
       "      <th>Information Security</th>\n",
       "      <th>Intellectual Property</th>\n",
       "      <th>Obscene, Degrading, and/or Abusive Content</th>\n",
       "      <th>Toxicity, Bias, and Homogenization</th>\n",
       "      <th>Value Chain and Component Integration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Risk                                                                    CBRN Information  \\\n",
       "0                                  Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "1                                               Big-bench: Self-Awareness - Benchmarking   \n",
       "2                                                 Big-bench: Truthfulness - Benchmarking   \n",
       "3     HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "4                                 HELM: Question answering, Summarization - Benchmarking   \n",
       "5                                                              Mark My Words - Benchmark   \n",
       "6                                                                                          \n",
       "7                                                                                          \n",
       "8                                                                                          \n",
       "9                                                                                          \n",
       "10                                                                                         \n",
       "11                                                                                         \n",
       "12                                                                                         \n",
       "13                                                                                         \n",
       "14                                                                                         \n",
       "15                                                                                         \n",
       "16                                                                                         \n",
       "17                                                                                         \n",
       "18                                                                                         \n",
       "19                                                                                         \n",
       "20                                                                                         \n",
       "21                                                                                         \n",
       "22                                                                                         \n",
       "23                                                                                         \n",
       "24                                                                                         \n",
       "25                                                                                         \n",
       "26                                                                                         \n",
       "27                                                                                         \n",
       "28                                                                                         \n",
       "29                                                                                         \n",
       "30                                                                                         \n",
       "31                                                                                         \n",
       "32                                                                                         \n",
       "33                                                                                         \n",
       "34                                                                                         \n",
       "35                                                                                         \n",
       "36                                                                                         \n",
       "37                                                                                         \n",
       "38                                                                                         \n",
       "39                                                                                         \n",
       "40                                                                                         \n",
       "41                                                                                         \n",
       "42                                                                                         \n",
       "43                                                                                         \n",
       "44                                                                                         \n",
       "45                                                                                         \n",
       "46                                                                                         \n",
       "47                                                                                         \n",
       "48                                                                                         \n",
       "49                                                                                         \n",
       "50                                                                                         \n",
       "51                                                                                         \n",
       "52                                                                                         \n",
       "53                                                                                         \n",
       "54                                                                                         \n",
       "55                                                                                         \n",
       "56                                                                                         \n",
       "57                                                                                         \n",
       "58                                                                                         \n",
       "59                                                                                         \n",
       "60                                                                                         \n",
       "61                                                                                         \n",
       "62                                                                                         \n",
       "63                                                                                         \n",
       "64                                                                                         \n",
       "65                                                                                         \n",
       "66                                                                                         \n",
       "67                                                                                         \n",
       "68                                                                                         \n",
       "69                                                                                         \n",
       "70                                                                                         \n",
       "71                                                                                         \n",
       "72                                                                                         \n",
       "73                                                                                         \n",
       "74                                                                                         \n",
       "75                                                                                         \n",
       "76                                                                                         \n",
       "77                                                                                         \n",
       "78                                                                                         \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                              Confabulation  \\\n",
       "0                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "1     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "2                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "3                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "4                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "5                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "6                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "7                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "8                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "9                                                                                                                                                                                                                         Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "10                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "11                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "12                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "13                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "14                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "15                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "16                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "17                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "19                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "20                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "21                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "22                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "23                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "24                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "25                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "26                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "27                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "28                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "29                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "30                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "31                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "32                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "33                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking   \n",
       "34                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "35                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "36                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "37                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "38                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "39                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "40                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "41                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "42                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "43                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "44                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "45                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "46                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "47                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "48                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "49                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "50                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "51                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "52                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "53                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "54                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "55                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "56                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "57                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "58                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "60                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "61                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "62                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "63                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "64                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "65                                                                                                                                                                                                                                                                                \n",
       "66                                                                                                                                                                                                                                                                                \n",
       "67                                                                                                                                                                                                                                                                                \n",
       "68                                                                                                                                                                                                                                                                                \n",
       "69                                                                                                                                                                                                                                                                                \n",
       "70                                                                                                                                                                                                                                                                                \n",
       "71                                                                                                                                                                                                                                                                                \n",
       "72                                                                                                                                                                                                                                                                                \n",
       "73                                                                                                                                                                                                                                                                                \n",
       "74                                                                                                                                                                                                                                                                                \n",
       "75                                                                                                                                                                                                                                                                                \n",
       "76                                                                                                                                                                                                                                                                                \n",
       "77                                                                                                                                                                                                                                                                                \n",
       "78                                                                                                                                                                                                                                                                                \n",
       "\n",
       "Risk                                                                 Dangerous or Violent Recommendations  \\\n",
       "0                                                   Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "1                                                                Big-bench: Self-Awareness - Benchmarking   \n",
       "2                                                                  Big-bench: Truthfulness - Benchmarking   \n",
       "3                           Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "4     DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "5                      HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "6                                                  HELM: Question answering, Summarization - Benchmarking   \n",
       "7                                                          In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "8                                                                           JailbreakingLLMs - Attack\\r\\n   \n",
       "9                                                                                    LLM Privacy - Attack   \n",
       "10                                                                                   MIMIR - Benchmarking   \n",
       "11                                                                              Mark My Words - Benchmark   \n",
       "12                                 TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "13                                                             detect-pretrain-code - Attack/Benchmarking   \n",
       "14                                                                                                          \n",
       "15                                                                                                          \n",
       "16                                                                                                          \n",
       "17                                                                                                          \n",
       "18                                                                                                          \n",
       "19                                                                                                          \n",
       "20                                                                                                          \n",
       "21                                                                                                          \n",
       "22                                                                                                          \n",
       "23                                                                                                          \n",
       "24                                                                                                          \n",
       "25                                                                                                          \n",
       "26                                                                                                          \n",
       "27                                                                                                          \n",
       "28                                                                                                          \n",
       "29                                                                                                          \n",
       "30                                                                                                          \n",
       "31                                                                                                          \n",
       "32                                                                                                          \n",
       "33                                                                                                          \n",
       "34                                                                                                          \n",
       "35                                                                                                          \n",
       "36                                                                                                          \n",
       "37                                                                                                          \n",
       "38                                                                                                          \n",
       "39                                                                                                          \n",
       "40                                                                                                          \n",
       "41                                                                                                          \n",
       "42                                                                                                          \n",
       "43                                                                                                          \n",
       "44                                                                                                          \n",
       "45                                                                                                          \n",
       "46                                                                                                          \n",
       "47                                                                                                          \n",
       "48                                                                                                          \n",
       "49                                                                                                          \n",
       "50                                                                                                          \n",
       "51                                                                                                          \n",
       "52                                                                                                          \n",
       "53                                                                                                          \n",
       "54                                                                                                          \n",
       "55                                                                                                          \n",
       "56                                                                                                          \n",
       "57                                                                                                          \n",
       "58                                                                                                          \n",
       "59                                                                                                          \n",
       "60                                                                                                          \n",
       "61                                                                                                          \n",
       "62                                                                                                          \n",
       "63                                                                                                          \n",
       "64                                                                                                          \n",
       "65                                                                                                          \n",
       "66                                                                                                          \n",
       "67                                                                                                          \n",
       "68                                                                                                          \n",
       "69                                                                                                          \n",
       "70                                                                                                          \n",
       "71                                                                                                          \n",
       "72                                                                                                          \n",
       "73                                                                                                          \n",
       "74                                                                                                          \n",
       "75                                                                                                          \n",
       "76                                                                                                          \n",
       "77                                                                                                          \n",
       "78                                                                                                          \n",
       "\n",
       "Risk                                                                                                                   Data Privacy  \\\n",
       "0     An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                             Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "2                                                                                          Big-bench: Self-Awareness - Benchmarking   \n",
       "3                                                                                            Big-bench: Truthfulness - Benchmarking   \n",
       "4                                                                                                       BloombergGPT - Benchmarking   \n",
       "5                                                     Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "6                               DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "7                                                          DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "8                                                                                         Evaluation Harness: ETHICS - Benchmarking   \n",
       "9                                                                                   HELM: Memorization and copyright - Benchmarking   \n",
       "10                                               HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "11                                                                           HELM: Question answering, Summarization - Benchmarking   \n",
       "12                                                                                   In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "13                                                                                                    JailbreakingLLMs - Attack\\r\\n   \n",
       "14                                                                                                             LLM Privacy - Attack   \n",
       "15                                                                   LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "16                                                                                                             MIMIR - Benchmarking   \n",
       "17                                                                                                        Mark My Words - Benchmark   \n",
       "18                                                           TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "19                                                                                       detect-pretrain-code - Attack/Benchmarking   \n",
       "20                                                                                                                                    \n",
       "21                                                                                                                                    \n",
       "22                                                                                                                                    \n",
       "23                                                                                                                                    \n",
       "24                                                                                                                                    \n",
       "25                                                                                                                                    \n",
       "26                                                                                                                                    \n",
       "27                                                                                                                                    \n",
       "28                                                                                                                                    \n",
       "29                                                                                                                                    \n",
       "30                                                                                                                                    \n",
       "31                                                                                                                                    \n",
       "32                                                                                                                                    \n",
       "33                                                                                                                                    \n",
       "34                                                                                                                                    \n",
       "35                                                                                                                                    \n",
       "36                                                                                                                                    \n",
       "37                                                                                                                                    \n",
       "38                                                                                                                                    \n",
       "39                                                                                                                                    \n",
       "40                                                                                                                                    \n",
       "41                                                                                                                                    \n",
       "42                                                                                                                                    \n",
       "43                                                                                                                                    \n",
       "44                                                                                                                                    \n",
       "45                                                                                                                                    \n",
       "46                                                                                                                                    \n",
       "47                                                                                                                                    \n",
       "48                                                                                                                                    \n",
       "49                                                                                                                                    \n",
       "50                                                                                                                                    \n",
       "51                                                                                                                                    \n",
       "52                                                                                                                                    \n",
       "53                                                                                                                                    \n",
       "54                                                                                                                                    \n",
       "55                                                                                                                                    \n",
       "56                                                                                                                                    \n",
       "57                                                                                                                                    \n",
       "58                                                                                                                                    \n",
       "59                                                                                                                                    \n",
       "60                                                                                                                                    \n",
       "61                                                                                                                                    \n",
       "62                                                                                                                                    \n",
       "63                                                                                                                                    \n",
       "64                                                                                                                                    \n",
       "65                                                                                                                                    \n",
       "66                                                                                                                                    \n",
       "67                                                                                                                                    \n",
       "68                                                                                                                                    \n",
       "69                                                                                                                                    \n",
       "70                                                                                                                                    \n",
       "71                                                                                                                                    \n",
       "72                                                                                                                                    \n",
       "73                                                                                                                                    \n",
       "74                                                                                                                                    \n",
       "75                                                                                                                                    \n",
       "76                                                                                                                                    \n",
       "77                                                                                                                                    \n",
       "78                                                                                                                                    \n",
       "\n",
       "Risk                                                                                                                                      Environmental  \\\n",
       "0                         An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                               BELEBELE - Benchmarking   \n",
       "2                                                                                                 Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "3                                                                             Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "4                                                                                                              Big-bench: Self-Awareness - Benchmarking   \n",
       "5                                                                       Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "6                                                                                                                    Big-bench: Toxicity - Benchmarking   \n",
       "7                                                                                                                Big-bench: Truthfulness - Benchmarking   \n",
       "8                                                                                                                           BloombergGPT - Benchmarking   \n",
       "9                                                                                                                DecodingTrust: Fairness - Benchmarking   \n",
       "10                                                                             DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "11                                                                                                        DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "12                                                                                                               DecodingTrust: Toxicity - Benchmarking   \n",
       "13                                                                                                 Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "14                                                          Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "15                                                                                                       Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "16                                                                                                            Evaluation Harness: ETHICS - Benchmarking   \n",
       "17                                                                                                           Evaluation Harness: ToxiGen - Benchmarking   \n",
       "18                                                              Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "19    From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "20                                                                                                                            HELM: Bias - Benchmarking   \n",
       "21                                                                                                          HELM: Language (Twitter AAE) - Benchmarking   \n",
       "22                                                                                                      HELM: Memorization and copyright - Benchmarking   \n",
       "23                                                                   HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "24                                                                                               HELM: Question answering, Summarization - Benchmarking   \n",
       "25                                                                                                                        HELM: Toxicity - Benchmarking   \n",
       "26                                                                                                              HELM: Toxicity detection - Benchmarking   \n",
       "27                                                                                       LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "28                                                                                                                               MASSIVE - Benchmarking   \n",
       "29                                                                                                                            Mark My Words - Benchmark   \n",
       "30                                                                                   The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "31                                                 Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "32                                                                                                                                                        \n",
       "33                                                                                                                                                        \n",
       "34                                                                                                                                                        \n",
       "35                                                                                                                                                        \n",
       "36                                                                                                                                                        \n",
       "37                                                                                                                                                        \n",
       "38                                                                                                                                                        \n",
       "39                                                                                                                                                        \n",
       "40                                                                                                                                                        \n",
       "41                                                                                                                                                        \n",
       "42                                                                                                                                                        \n",
       "43                                                                                                                                                        \n",
       "44                                                                                                                                                        \n",
       "45                                                                                                                                                        \n",
       "46                                                                                                                                                        \n",
       "47                                                                                                                                                        \n",
       "48                                                                                                                                                        \n",
       "49                                                                                                                                                        \n",
       "50                                                                                                                                                        \n",
       "51                                                                                                                                                        \n",
       "52                                                                                                                                                        \n",
       "53                                                                                                                                                        \n",
       "54                                                                                                                                                        \n",
       "55                                                                                                                                                        \n",
       "56                                                                                                                                                        \n",
       "57                                                                                                                                                        \n",
       "58                                                                                                                                                        \n",
       "59                                                                                                                                                        \n",
       "60                                                                                                                                                        \n",
       "61                                                                                                                                                        \n",
       "62                                                                                                                                                        \n",
       "63                                                                                                                                                        \n",
       "64                                                                                                                                                        \n",
       "65                                                                                                                                                        \n",
       "66                                                                                                                                                        \n",
       "67                                                                                                                                                        \n",
       "68                                                                                                                                                        \n",
       "69                                                                                                                                                        \n",
       "70                                                                                                                                                        \n",
       "71                                                                                                                                                        \n",
       "72                                                                                                                                                        \n",
       "73                                                                                                                                                        \n",
       "74                                                                                                                                                        \n",
       "75                                                                                                                                                        \n",
       "76                                                                                                                                                        \n",
       "77                                                                                                                                                        \n",
       "78                                                                                                                                                        \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                     Human-AI Configuration  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                      Information Integrity  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "2                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "3                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "4                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "5                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "6                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "7                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "8                                                                                                                                                                                                                         Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "9                                                                                                                                                                                                                      Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "10                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "11                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "12                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "13                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "14                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "15                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "16                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "17                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "18                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "19                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "20                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "21                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "22                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "23                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "24                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "25                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "26                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "27                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "28                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking   \n",
       "29                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "30                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "31                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "32                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "33                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "34                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "35                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "36                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "37                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "38                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "39                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "40                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "41                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "42                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "43                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "44                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "45                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "46                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "47                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "48                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "49                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "50                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "51                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "52                                                                                                                                                                                                                                                                                \n",
       "53                                                                                                                                                                                                                                                                                \n",
       "54                                                                                                                                                                                                                                                                                \n",
       "55                                                                                                                                                                                                                                                                                \n",
       "56                                                                                                                                                                                                                                                                                \n",
       "57                                                                                                                                                                                                                                                                                \n",
       "58                                                                                                                                                                                                                                                                                \n",
       "59                                                                                                                                                                                                                                                                                \n",
       "60                                                                                                                                                                                                                                                                                \n",
       "61                                                                                                                                                                                                                                                                                \n",
       "62                                                                                                                                                                                                                                                                                \n",
       "63                                                                                                                                                                                                                                                                                \n",
       "64                                                                                                                                                                                                                                                                                \n",
       "65                                                                                                                                                                                                                                                                                \n",
       "66                                                                                                                                                                                                                                                                                \n",
       "67                                                                                                                                                                                                                                                                                \n",
       "68                                                                                                                                                                                                                                                                                \n",
       "69                                                                                                                                                                                                                                                                                \n",
       "70                                                                                                                                                                                                                                                                                \n",
       "71                                                                                                                                                                                                                                                                                \n",
       "72                                                                                                                                                                                                                                                                                \n",
       "73                                                                                                                                                                                                                                                                                \n",
       "74                                                                                                                                                                                                                                                                                \n",
       "75                                                                                                                                                                                                                                                                                \n",
       "76                                                                                                                                                                                                                                                                                \n",
       "77                                                                                                                                                                                                                                                                                \n",
       "78                                                                                                                                                                                                                                                                                \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                       Information Security  \\\n",
       "0     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "1                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "2                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "3                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "4                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "5                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "6                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "7                                                                                                                                                                                                                         Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "8                                                                                                                                                                                                                      Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "9                                                                                                                                                                                                                                          Big-bench: Paraphrase - Benchmarking   \n",
       "10                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "11                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "12                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "13                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "14                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "15                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "16                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "17                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "18                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "19                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "20                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "21                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "22                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "23                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "24                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "25                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "26                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking   \n",
       "27                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "28                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "29                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "30                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "31                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "32                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "33                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "34                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "35                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "36                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "37                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "38                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "39                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "40                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "41                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "42                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "43                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "44                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "45                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "46                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "47                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "48                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "49                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "50                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "51                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "52                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "53                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "54                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "55                                                                                                                                                                                                                                                                                \n",
       "56                                                                                                                                                                                                                                                                                \n",
       "57                                                                                                                                                                                                                                                                                \n",
       "58                                                                                                                                                                                                                                                                                \n",
       "59                                                                                                                                                                                                                                                                                \n",
       "60                                                                                                                                                                                                                                                                                \n",
       "61                                                                                                                                                                                                                                                                                \n",
       "62                                                                                                                                                                                                                                                                                \n",
       "63                                                                                                                                                                                                                                                                                \n",
       "64                                                                                                                                                                                                                                                                                \n",
       "65                                                                                                                                                                                                                                                                                \n",
       "66                                                                                                                                                                                                                                                                                \n",
       "67                                                                                                                                                                                                                                                                                \n",
       "68                                                                                                                                                                                                                                                                                \n",
       "69                                                                                                                                                                                                                                                                                \n",
       "70                                                                                                                                                                                                                                                                                \n",
       "71                                                                                                                                                                                                                                                                                \n",
       "72                                                                                                                                                                                                                                                                                \n",
       "73                                                                                                                                                                                                                                                                                \n",
       "74                                                                                                                                                                                                                                                                                \n",
       "75                                                                                                                                                                                                                                                                                \n",
       "76                                                                                                                                                                                                                                                                                \n",
       "77                                                                                                                                                                                                                                                                                \n",
       "78                                                                                                                                                                                                                                                                                \n",
       "\n",
       "Risk                                                                                                                              Intellectual Property  \\\n",
       "0                         An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                               BELEBELE - Benchmarking   \n",
       "2                                                                             Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "3                                                                       Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "4                                                                                                                    Big-bench: Toxicity - Benchmarking   \n",
       "5                                                                                                                           BloombergGPT - Benchmarking   \n",
       "6                                                                                                                DecodingTrust: Fairness - Benchmarking   \n",
       "7                                                                              DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "8                                                                                                         DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "9                                                                                                                DecodingTrust: Toxicity - Benchmarking   \n",
       "10                                                                                                 Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "11                                                          Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "12                                                                                                       Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "13                                                                                                            Evaluation Harness: ETHICS - Benchmarking   \n",
       "14                                                                                                           Evaluation Harness: ToxiGen - Benchmarking   \n",
       "15                                                              Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "16    From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "17                                                                                                                            HELM: Bias - Benchmarking   \n",
       "18                                                                                                          HELM: Language (Twitter AAE) - Benchmarking   \n",
       "19                                                                                                      HELM: Memorization and copyright - Benchmarking   \n",
       "20                                                                                                                        HELM: Toxicity - Benchmarking   \n",
       "21                                                                                                              HELM: Toxicity detection - Benchmarking   \n",
       "22                                                                                                                                 LLM Privacy - Attack   \n",
       "23                                                                                       LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "24                                                                                                                               MASSIVE - Benchmarking   \n",
       "25                                                                                                                                 MIMIR - Benchmarking   \n",
       "26                                                                                   The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "27                                                 Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "28                                                                                                                                                        \n",
       "29                                                                                                                                                        \n",
       "30                                                                                                                                                        \n",
       "31                                                                                                                                                        \n",
       "32                                                                                                                                                        \n",
       "33                                                                                                                                                        \n",
       "34                                                                                                                                                        \n",
       "35                                                                                                                                                        \n",
       "36                                                                                                                                                        \n",
       "37                                                                                                                                                        \n",
       "38                                                                                                                                                        \n",
       "39                                                                                                                                                        \n",
       "40                                                                                                                                                        \n",
       "41                                                                                                                                                        \n",
       "42                                                                                                                                                        \n",
       "43                                                                                                                                                        \n",
       "44                                                                                                                                                        \n",
       "45                                                                                                                                                        \n",
       "46                                                                                                                                                        \n",
       "47                                                                                                                                                        \n",
       "48                                                                                                                                                        \n",
       "49                                                                                                                                                        \n",
       "50                                                                                                                                                        \n",
       "51                                                                                                                                                        \n",
       "52                                                                                                                                                        \n",
       "53                                                                                                                                                        \n",
       "54                                                                                                                                                        \n",
       "55                                                                                                                                                        \n",
       "56                                                                                                                                                        \n",
       "57                                                                                                                                                        \n",
       "58                                                                                                                                                        \n",
       "59                                                                                                                                                        \n",
       "60                                                                                                                                                        \n",
       "61                                                                                                                                                        \n",
       "62                                                                                                                                                        \n",
       "63                                                                                                                                                        \n",
       "64                                                                                                                                                        \n",
       "65                                                                                                                                                        \n",
       "66                                                                                                                                                        \n",
       "67                                                                                                                                                        \n",
       "68                                                                                                                                                        \n",
       "69                                                                                                                                                        \n",
       "70                                                                                                                                                        \n",
       "71                                                                                                                                                        \n",
       "72                                                                                                                                                        \n",
       "73                                                                                                                                                        \n",
       "74                                                                                                                                                        \n",
       "75                                                                                                                                                        \n",
       "76                                                                                                                                                        \n",
       "77                                                                                                                                                        \n",
       "78                                                                                                                                                        \n",
       "\n",
       "Risk                                                                                                         Obscene, Degrading, and/or Abusive Content  \\\n",
       "0                                                                                                                               BELEBELE - Benchmarking   \n",
       "1                                                                                                 Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "2                                                                             Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "3                                                                                                              Big-bench: Self-Awareness - Benchmarking   \n",
       "4                                                                       Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "5                                                                                                                    Big-bench: Toxicity - Benchmarking   \n",
       "6                                                                                                                Big-bench: Truthfulness - Benchmarking   \n",
       "7                                                                                                                DecodingTrust: Fairness - Benchmarking   \n",
       "8                                                                                                         DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "9                                                                                                                DecodingTrust: Toxicity - Benchmarking   \n",
       "10                                                                                                 Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "11                                                          Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "12                                                                                                       Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "13                                                                                                           Evaluation Harness: ToxiGen - Benchmarking   \n",
       "14                                                              Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "15    From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "16                                                                                                                            HELM: Bias - Benchmarking   \n",
       "17                                                                                                          HELM: Language (Twitter AAE) - Benchmarking   \n",
       "18                                                                   HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "19                                                                                               HELM: Question answering, Summarization - Benchmarking   \n",
       "20                                                                                                                        HELM: Toxicity - Benchmarking   \n",
       "21                                                                                                              HELM: Toxicity detection - Benchmarking   \n",
       "22                                                                                                                               MASSIVE - Benchmarking   \n",
       "23                                                                                                                            Mark My Words - Benchmark   \n",
       "24                                                                                   The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "25                                                 Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "26                                                                                                                                                        \n",
       "27                                                                                                                                                        \n",
       "28                                                                                                                                                        \n",
       "29                                                                                                                                                        \n",
       "30                                                                                                                                                        \n",
       "31                                                                                                                                                        \n",
       "32                                                                                                                                                        \n",
       "33                                                                                                                                                        \n",
       "34                                                                                                                                                        \n",
       "35                                                                                                                                                        \n",
       "36                                                                                                                                                        \n",
       "37                                                                                                                                                        \n",
       "38                                                                                                                                                        \n",
       "39                                                                                                                                                        \n",
       "40                                                                                                                                                        \n",
       "41                                                                                                                                                        \n",
       "42                                                                                                                                                        \n",
       "43                                                                                                                                                        \n",
       "44                                                                                                                                                        \n",
       "45                                                                                                                                                        \n",
       "46                                                                                                                                                        \n",
       "47                                                                                                                                                        \n",
       "48                                                                                                                                                        \n",
       "49                                                                                                                                                        \n",
       "50                                                                                                                                                        \n",
       "51                                                                                                                                                        \n",
       "52                                                                                                                                                        \n",
       "53                                                                                                                                                        \n",
       "54                                                                                                                                                        \n",
       "55                                                                                                                                                        \n",
       "56                                                                                                                                                        \n",
       "57                                                                                                                                                        \n",
       "58                                                                                                                                                        \n",
       "59                                                                                                                                                        \n",
       "60                                                                                                                                                        \n",
       "61                                                                                                                                                        \n",
       "62                                                                                                                                                        \n",
       "63                                                                                                                                                        \n",
       "64                                                                                                                                                        \n",
       "65                                                                                                                                                        \n",
       "66                                                                                                                                                        \n",
       "67                                                                                                                                                        \n",
       "68                                                                                                                                                        \n",
       "69                                                                                                                                                        \n",
       "70                                                                                                                                                        \n",
       "71                                                                                                                                                        \n",
       "72                                                                                                                                                        \n",
       "73                                                                                                                                                        \n",
       "74                                                                                                                                                        \n",
       "75                                                                                                                                                        \n",
       "76                                                                                                                                                        \n",
       "77                                                                                                                                                        \n",
       "78                                                                                                                                                        \n",
       "\n",
       "Risk                                                                                                                                                                                                                                         Toxicity, Bias, and Homogenization  \\\n",
       "0                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "1     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "2                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "3                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "4                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "5                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "6                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "7                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "8                                                                                                                                                                                                                         Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "9                                                                                                                                                                                                                      Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "10                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "11                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "12                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "13                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "14                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "15                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "16                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "17                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "19                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "20                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "21                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "22                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "23                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "24                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "25                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "26                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "27                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "28                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "29                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "30                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking   \n",
       "31                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "32                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "33                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "34                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "35                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "36                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "37                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "38                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "39                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "40                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "41                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "42                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "43                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "44                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "45                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "46                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "48                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "50                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "51                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "52                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "53                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "54                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "55                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "56                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "57                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "58                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                                                \n",
       "60                                                                                                                                                                                                                                                                                \n",
       "61                                                                                                                                                                                                                                                                                \n",
       "62                                                                                                                                                                                                                                                                                \n",
       "63                                                                                                                                                                                                                                                                                \n",
       "64                                                                                                                                                                                                                                                                                \n",
       "65                                                                                                                                                                                                                                                                                \n",
       "66                                                                                                                                                                                                                                                                                \n",
       "67                                                                                                                                                                                                                                                                                \n",
       "68                                                                                                                                                                                                                                                                                \n",
       "69                                                                                                                                                                                                                                                                                \n",
       "70                                                                                                                                                                                                                                                                                \n",
       "71                                                                                                                                                                                                                                                                                \n",
       "72                                                                                                                                                                                                                                                                                \n",
       "73                                                                                                                                                                                                                                                                                \n",
       "74                                                                                                                                                                                                                                                                                \n",
       "75                                                                                                                                                                                                                                                                                \n",
       "76                                                                                                                                                                                                                                                                                \n",
       "77                                                                                                                                                                                                                                                                                \n",
       "78                                                                                                                                                                                                                                                                                \n",
       "\n",
       "Risk                                                                                                                                                                                                                                      Value Chain and Component Integration  \n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)  \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking  \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking  \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking  \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking  \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking  \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking  \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking  \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking  \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking  \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking  \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking  \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking  \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking  \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking  \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking  \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking  \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking  \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking  \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking  \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack  \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking  \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)  \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking  \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking  \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking  \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking  \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking  \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking  \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking  \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking  \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking  \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking  \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking  \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking  \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking  \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking  \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking  \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking  \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)  \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)  \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)  \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)  \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking  \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking  \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking  \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking  \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking  \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking  \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking  \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking  \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)  \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking  \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking  \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking  \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking  \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking  \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking  \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking  \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking  \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking  \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking  \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking  \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking  \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack  \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n  \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack  \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)  \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking  \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking  \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)  \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark  \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)  \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack  \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking  \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking  \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_risk_measure_by_gai_risk_frame = pd.DataFrame(columns=data_dict['gai_risk']['Risk'], index=range(0, 79))\n",
    "\n",
    "for j, risk in enumerate(low_risk_measure_by_gai_risk_frame.columns):\n",
    "    for i, eval_ in enumerate(list(intermediate_dict[risk])):     \n",
    "        low_risk_measure_by_gai_risk_frame.iloc[i, j] = eval_\n",
    "    if low_risk_measure_by_gai_risk_frame[risk].isnull().any().any():\n",
    "            low_risk_measure_by_gai_risk_frame[risk] = low_risk_measure_by_gai_risk_frame[risk].replace(np.nan, '')\n",
    "\n",
    "low_risk_measure_by_gai_risk_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b829c538-186b-41c5-9d86-dd6b9260d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "CBRN Information \\\\\n",
      "\\midrule\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Confabulation \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Dangerous or Violent Recommendations \\\\\n",
      "\\midrule\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Data Privacy \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Environmental \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Human-AI Configuration \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Information Integrity \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Information Security \\\\\n",
      "\\midrule\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Intellectual Property \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Obscene, Degrading, and/or Abusive Content \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Toxicity, Bias, and Homogenization \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Value Chain and Component Integration \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual – Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for risk in low_risk_measure_by_gai_risk_frame.columns:\n",
    "    print(low_risk_measure_by_gai_risk_frame[risk].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d3da1-7818-4347-818c-9c654f28b94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
