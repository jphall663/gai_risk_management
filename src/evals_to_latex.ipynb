{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7ad31e1-1f05-4c9b-a3de-bc21fda68f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# show everything\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66ca2f43-89af-4d0e-8552-f4cc3e46099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accountable and Transparent</th>\n",
       "      <th>Explainable and Interpretable</th>\n",
       "      <th>Fair with Harmful Bias Managed</th>\n",
       "      <th>Privacy Enhanced</th>\n",
       "      <th>Safe</th>\n",
       "      <th>Secure and Resilient</th>\n",
       "      <th>Valid and Reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td></td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      Accountable and Transparent  \\\n",
       "0   An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                     BloombergGPT - Benchmarking   \n",
       "2                                                        DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "3                                                                                       Evaluation Harness: ETHICS - Benchmarking   \n",
       "4                                                                                 HELM: Memorization and copyright - Benchmarking   \n",
       "5                                                                  LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "6                                                                                                                                   \n",
       "7                                                                                                                                   \n",
       "8                                                                                                                                   \n",
       "9                                                                                                                                   \n",
       "10                                                                                                                                  \n",
       "11                                                                                                                                  \n",
       "12                                                                                                                                  \n",
       "13                                                                                                                                  \n",
       "14                                                                                                                                  \n",
       "15                                                                                                                                  \n",
       "16                                                                                                                                  \n",
       "17                                                                                                                                  \n",
       "18                                                                                                                                  \n",
       "19                                                                                                                                  \n",
       "20                                                                                                                                  \n",
       "21                                                                                                                                  \n",
       "22                                                                                                                                  \n",
       "23                                                                                                                                  \n",
       "24                                                                                                                                  \n",
       "25                                                                                                                                  \n",
       "26                                                                                                                                  \n",
       "27                                                                                                                                  \n",
       "28                                                                                                                                  \n",
       "29                                                                                                                                  \n",
       "30                                                                                                                                  \n",
       "31                                                                                                                                  \n",
       "32                                                                                                                                  \n",
       "33                                                                                                                                  \n",
       "34                                                                                                                                  \n",
       "35                                                                                                                                  \n",
       "36                                                                                                                                  \n",
       "37                                                                                                                                  \n",
       "38                                                                                                                                  \n",
       "39                                                                                                                                  \n",
       "\n",
       "   Explainable and Interpretable  \\\n",
       "0                                  \n",
       "1                                  \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "5                                  \n",
       "6                                  \n",
       "7                                  \n",
       "8                                  \n",
       "9                                  \n",
       "10                                 \n",
       "11                                 \n",
       "12                                 \n",
       "13                                 \n",
       "14                                 \n",
       "15                                 \n",
       "16                                 \n",
       "17                                 \n",
       "18                                 \n",
       "19                                 \n",
       "20                                 \n",
       "21                                 \n",
       "22                                 \n",
       "23                                 \n",
       "24                                 \n",
       "25                                 \n",
       "26                                 \n",
       "27                                 \n",
       "28                                 \n",
       "29                                 \n",
       "30                                 \n",
       "31                                 \n",
       "32                                 \n",
       "33                                 \n",
       "34                                 \n",
       "35                                 \n",
       "36                                 \n",
       "37                                 \n",
       "38                                 \n",
       "39                                 \n",
       "\n",
       "                                                                                                                       Fair with Harmful Bias Managed  \\\n",
       "0                                                                                                                             BELEBELE - Benchmarking   \n",
       "1                                                                           Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "2                                                                     Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "3                                                                                                                  Big-bench: Toxicity - Benchmarking   \n",
       "4                                                                                                              DecodingTrust: Fairness - Benchmarking   \n",
       "5                                                                                                       DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "6                                                                                                              DecodingTrust: Toxicity - Benchmarking   \n",
       "7                                                                                                Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "8                                                         Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "9                                                                                                      Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "10                                                                                                         Evaluation Harness: ToxiGen - Benchmarking   \n",
       "11                                                            Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "12  From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "13                                                                                                                          HELM: Bias - Benchmarking   \n",
       "14                                                                                                        HELM: Language (Twitter AAE) - Benchmarking   \n",
       "15                                                                                                                      HELM: Toxicity - Benchmarking   \n",
       "16                                                                                                            HELM: Toxicity detection - Benchmarking   \n",
       "17                                                                                                                             MASSIVE - Benchmarking   \n",
       "18                                                                                 The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "19                                               Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "20                                                                                                                                                      \n",
       "21                                                                                                                                                      \n",
       "22                                                                                                                                                      \n",
       "23                                                                                                                                                      \n",
       "24                                                                                                                                                      \n",
       "25                                                                                                                                                      \n",
       "26                                                                                                                                                      \n",
       "27                                                                                                                                                      \n",
       "28                                                                                                                                                      \n",
       "29                                                                                                                                                      \n",
       "30                                                                                                                                                      \n",
       "31                                                                                                                                                      \n",
       "32                                                                                                                                                      \n",
       "33                                                                                                                                                      \n",
       "34                                                                                                                                                      \n",
       "35                                                                                                                                                      \n",
       "36                                                                                                                                                      \n",
       "37                                                                                                                                                      \n",
       "38                                                                                                                                                      \n",
       "39                                                                                                                                                      \n",
       "\n",
       "                                   Privacy Enhanced  \\\n",
       "0   HELM: Memorization and copyright - Benchmarking   \n",
       "1                              LLM Privacy - Attack   \n",
       "2                              MIMIR - Benchmarking   \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                                     \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     \n",
       "10                                                    \n",
       "11                                                    \n",
       "12                                                    \n",
       "13                                                    \n",
       "14                                                    \n",
       "15                                                    \n",
       "16                                                    \n",
       "17                                                    \n",
       "18                                                    \n",
       "19                                                    \n",
       "20                                                    \n",
       "21                                                    \n",
       "22                                                    \n",
       "23                                                    \n",
       "24                                                    \n",
       "25                                                    \n",
       "26                                                    \n",
       "27                                                    \n",
       "28                                                    \n",
       "29                                                    \n",
       "30                                                    \n",
       "31                                                    \n",
       "32                                                    \n",
       "33                                                    \n",
       "34                                                    \n",
       "35                                                    \n",
       "36                                                    \n",
       "37                                                    \n",
       "38                                                    \n",
       "39                                                    \n",
       "\n",
       "                                                                                  Safe  \\\n",
       "0                                Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "1                                             Big-bench: Self-Awareness - Benchmarking   \n",
       "2                                               Big-bench: Truthfulness - Benchmarking   \n",
       "3   HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "4                               HELM: Question answering, Summarization - Benchmarking   \n",
       "5                                                            Mark My Words - Benchmark   \n",
       "6                                                                                        \n",
       "7                                                                                        \n",
       "8                                                                                        \n",
       "9                                                                                        \n",
       "10                                                                                       \n",
       "11                                                                                       \n",
       "12                                                                                       \n",
       "13                                                                                       \n",
       "14                                                                                       \n",
       "15                                                                                       \n",
       "16                                                                                       \n",
       "17                                                                                       \n",
       "18                                                                                       \n",
       "19                                                                                       \n",
       "20                                                                                       \n",
       "21                                                                                       \n",
       "22                                                                                       \n",
       "23                                                                                       \n",
       "24                                                                                       \n",
       "25                                                                                       \n",
       "26                                                                                       \n",
       "27                                                                                       \n",
       "28                                                                                       \n",
       "29                                                                                       \n",
       "30                                                                                       \n",
       "31                                                                                       \n",
       "32                                                                                       \n",
       "33                                                                                       \n",
       "34                                                                                       \n",
       "35                                                                                       \n",
       "36                                                                                       \n",
       "37                                                                                       \n",
       "38                                                                                       \n",
       "39                                                                                       \n",
       "\n",
       "                                                                                   Secure and Resilient  \\\n",
       "0                         Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "1   DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "2                                                            detect-pretrain-code - Attack/Benchmarking   \n",
       "3                                                        In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "4                                                                         JailbreakingLLMs - Attack\\r\\n   \n",
       "5                                                                                  LLM Privacy - Attack   \n",
       "6                                                                             Mark My Words - Benchmark   \n",
       "7                                                                                  MIMIR - Benchmarking   \n",
       "8                                TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "9                                                                                                         \n",
       "10                                                                                                        \n",
       "11                                                                                                        \n",
       "12                                                                                                        \n",
       "13                                                                                                        \n",
       "14                                                                                                        \n",
       "15                                                                                                        \n",
       "16                                                                                                        \n",
       "17                                                                                                        \n",
       "18                                                                                                        \n",
       "19                                                                                                        \n",
       "20                                                                                                        \n",
       "21                                                                                                        \n",
       "22                                                                                                        \n",
       "23                                                                                                        \n",
       "24                                                                                                        \n",
       "25                                                                                                        \n",
       "26                                                                                                        \n",
       "27                                                                                                        \n",
       "28                                                                                                        \n",
       "29                                                                                                        \n",
       "30                                                                                                        \n",
       "31                                                                                                        \n",
       "32                                                                                                        \n",
       "33                                                                                                        \n",
       "34                                                                                                        \n",
       "35                                                                                                        \n",
       "36                                                                                                        \n",
       "37                                                                                                        \n",
       "38                                                                                                        \n",
       "39                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                           Valid and Reliable  \n",
       "0   Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking  \n",
       "1                                                                                                             Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking  \n",
       "2                                                                                                                                                                                                                   Big-bench: Context Free Question Answering - Benchmarking  \n",
       "3                                                                                                                                                                         Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking  \n",
       "4                                                                                                                                                                                                                                        Big-bench: Creativity - Benchmarking  \n",
       "5                                                                                                                                                                                                Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking  \n",
       "6                                                                                                                                                                                                                       Big-bench: Morphology, Grammar, Syntax - Benchmarking  \n",
       "7                                                                                                                                                                                                                    Big-bench: Out-of-Distribution Robustness - Benchmarking  \n",
       "8                                                                                                                                                                                                                                        Big-bench: Paraphrase - Benchmarking  \n",
       "9                                                                                                                                                                                                                            Big-bench: Sufficient information - Benchmarking  \n",
       "10                                                                                                                                                                                                                                    Big-bench: Summarization - Benchmarking  \n",
       "11                                                                                                                                        DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "12                                                                                                                                                                                                                      Eval Gauntlet\\r\\nReading comprehension - Benchmarking  \n",
       "13                                                                                                                                                                                 Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking  \n",
       "14                                                                                                                                                                                                                       Eval Gauntlet: Language Understanding - Benchmarking  \n",
       "15                                                                                                                                                                                                                              Eval Gauntlet: World Knowledge - Benchmarking  \n",
       "16                                                                                                                                                                                                                                   Evaluation Harness: BLiMP - Benchmarking  \n",
       "17                                                                                                                                                                                                                               Evaluation Harness: CoQA, ARC - Benchmarking  \n",
       "18                                                                                                                                                                                                                                    Evaluation Harness: GLUE - Benchmarking  \n",
       "19                                                                                                                                             Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking  \n",
       "20                                                                                                                                                                                                                                  Evaluation Harness: MuTual - Benchmarking  \n",
       "21              Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking  \n",
       "22                                                                                                                                                                                                  FLASK: Background Knowledge - Benchmarking (with human and model scoring)  \n",
       "23                                                                                                                              FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)  \n",
       "24                                                                                                                                                                                                         FLASK: Metacognition - Benchmarking (with human and model scoring)  \n",
       "25                                                                                                                                                                              FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)  \n",
       "26                                                                                                                                                                                                                                             HELM: Knowledge - Benchmarking  \n",
       "27                                                                                                                                                                                                                                              HELM: Language - Benchmarking  \n",
       "28                                                                                                                                                                                                                     HELM: Miscellaneous text classification - Benchmarking  \n",
       "29                                                                                                                                                                                                                                    HELM: Question answering - Benchmarking  \n",
       "30                                                                                                                                                                                                                                             HELM: Reasoning - Benchmarking  \n",
       "31                                                                                                                                                                                                                           HELM: Robustness to contrast sets - Benchmarking  \n",
       "32                                                                                                                                                                                                                                         HELM: Summarization - Benchmarking  \n",
       "33                                                                                                                                                                                                                                Hugging Face: Conversational - Benchmarking  \n",
       "34                                                                                                                                                                                                                    Hugging Face: Fill-mask, Text generation - Benchmarking  \n",
       "35                                                                                                                                                                                                                            Hugging Face: Question answering - Benchmarking  \n",
       "36                                                                                                                                                                                                                                 Hugging Face: Summarization - Benchmarking  \n",
       "37                                                                                                                                                                           Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking  \n",
       "38                                                                                                                                                                                                                     MT-bench - Benchmarking (with human and model scoring)  \n",
       "39                                                                                                                                                                              Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'..' + os.sep + 'data' + os.sep + 'data.pkl', 'rb') as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "data_dict['low_risk_measure_by_tc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41a8864a-22b6-400a-b5c1-400667050274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Accountable and Transparent \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Explainable and Interpretable \\\\\n",
      "\\midrule\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Fair with Harmful Bias Managed \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Privacy Enhanced \\\\\n",
      "\\midrule\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Safe \\\\\n",
      "\\midrule\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Secure and Resilient \\\\\n",
      "\\midrule\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Valid and Reliable \\\\\n",
      "\\midrule\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tc in data_dict['low_risk_measure_by_tc'].columns:\n",
    "    print(data_dict['low_risk_measure_by_tc'][tc].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9237a45e-fc52-4370-8263-172b1eecccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Fair with Harmful Bias Managed \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_dict['low_risk_measure_by_tc']['Fair with Harmful Bias Managed'].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6e5df13-08e1-4c59-a104-6c2942610e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': ['Safe', 'Secure and Resilient'],\n",
       " 'Confabulation': ['Accountable and Transparent',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Dangerous or Violent Recommendations': ['Accountable and Transparent',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Data Privacy': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Environmental': ['Accountable and Transparent',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Valid and Reliable'],\n",
       " 'Human-AI Configuration': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Information Integrity': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Information Security': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Intellectual Property': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Obscene, Degrading, and/or Abusive Content': ['Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe'],\n",
       " 'Toxicity, Bias, and Homogenization': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable'],\n",
       " 'Value Chain and Component Integration': ['Accountable and Transparent',\n",
       "  'Explainable and Interpretable',\n",
       "  'Fair with Harmful Bias Managed',\n",
       "  'Privacy Enhanced',\n",
       "  'Safe',\n",
       "  'Secure and Resilient',\n",
       "  'Valid and Reliable']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'..' + os.sep + 'data' + os.sep + 'gai_risk_to_tc_map.pkl', 'rb') as fp:\n",
    "    gai_risk_to_tc_map = pickle.load(fp)\n",
    "\n",
    "gai_risk_to_tc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17d44fae-1c82-4e95-968e-53c5ab727dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': None,\n",
       " 'Confabulation': None,\n",
       " 'Dangerous or Violent Recommendations': None,\n",
       " 'Data Privacy': None,\n",
       " 'Environmental': None,\n",
       " 'Human-AI Configuration': None,\n",
       " 'Information Integrity': None,\n",
       " 'Information Security': None,\n",
       " 'Intellectual Property': None,\n",
       " 'Obscene, Degrading, and/or Abusive Content': None,\n",
       " 'Toxicity, Bias, and Homogenization': None,\n",
       " 'Value Chain and Component Integration': None}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_dict = {key: None for key in list(data_dict['gai_risk']['Risk'])}\n",
    "intermediate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb98589a-965e-4828-beae-8fff7e2b1da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBRN Information 14\n",
      "Confabulation 79\n",
      "Dangerous or Violent Recommendations 79\n",
      "Data Privacy 79\n",
      "Environmental 65\n",
      "Human-AI Configuration 79\n",
      "Information Integrity 79\n",
      "Information Security 79\n",
      "Intellectual Property 79\n",
      "Obscene, Degrading, and/or Abusive Content 29\n",
      "Toxicity, Bias, and Homogenization 79\n",
      "Value Chain and Component Integration 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CBRN Information': ['Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Confabulation': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Dangerous or Violent Recommendations': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Data Privacy': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Environmental': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Human-AI Configuration': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Information Integrity': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Information Security': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Intellectual Property': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Obscene, Degrading, and/or Abusive Content': ['BELEBELE - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'LLM Privacy - Attack',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'Mark My Words - Benchmark',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking'],\n",
       " 'Toxicity, Bias, and Homogenization': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking'],\n",
       " 'Value Chain and Component Integration': ['An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)',\n",
       "  'BELEBELE - Benchmarking',\n",
       "  'Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking',\n",
       "  'Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking',\n",
       "  'Big-bench: Context Free Question Answering - Benchmarking',\n",
       "  'Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking',\n",
       "  'Big-bench: Convince Me (specific task) - Benchmarking',\n",
       "  'Big-bench: Creativity - Benchmarking',\n",
       "  'Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking',\n",
       "  'Big-bench: Low-resource language, Non-English, Translation - Benchmarking',\n",
       "  'Big-bench: Morphology, Grammar, Syntax - Benchmarking',\n",
       "  'Big-bench: Out-of-Distribution Robustness - Benchmarking',\n",
       "  'Big-bench: Paraphrase - Benchmarking',\n",
       "  'Big-bench: Self-Awareness - Benchmarking',\n",
       "  'Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking',\n",
       "  'Big-bench: Sufficient information - Benchmarking',\n",
       "  'Big-bench: Summarization - Benchmarking',\n",
       "  'Big-bench: Toxicity - Benchmarking',\n",
       "  'Big-bench: Truthfulness - Benchmarking',\n",
       "  'BloombergGPT - Benchmarking',\n",
       "  'Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack',\n",
       "  'DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Fairness - Benchmarking',\n",
       "  'DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)',\n",
       "  'DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking',\n",
       "  'DecodingTrust: Stereotype Bias - Benchmarking',\n",
       "  'DecodingTrust: Toxicity - Benchmarking',\n",
       "  'Eval Gauntlet\\r\\nReading comprehension - Benchmarking',\n",
       "  'Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking',\n",
       "  'Eval Gauntlet: Language Understanding - Benchmarking',\n",
       "  'Eval Gauntlet: World Knowledge - Benchmarking',\n",
       "  'Evaluation Harness: BLiMP - Benchmarking',\n",
       "  'Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking',\n",
       "  'Evaluation Harness: CoQA, ARC - Benchmarking',\n",
       "  'Evaluation Harness: CrowS-Pairs - Benchmarking',\n",
       "  'Evaluation Harness: ETHICS - Benchmarking',\n",
       "  'Evaluation Harness: GLUE - Benchmarking',\n",
       "  'Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking',\n",
       "  'Evaluation Harness: MuTual - Benchmarking',\n",
       "  'Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking',\n",
       "  'Evaluation Harness: ToxiGen - Benchmarking',\n",
       "  'FLASK: Background Knowledge - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Metacognition - Benchmarking (with human and model scoring)',\n",
       "  'FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)',\n",
       "  'Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking',\n",
       "  'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking',\n",
       "  'HELM: Bias - Benchmarking',\n",
       "  'HELM: Knowledge - Benchmarking',\n",
       "  'HELM: Language (Twitter AAE) - Benchmarking',\n",
       "  'HELM: Language - Benchmarking',\n",
       "  'HELM: Memorization and copyright - Benchmarking',\n",
       "  'HELM: Miscellaneous text classification - Benchmarking',\n",
       "  'HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)',\n",
       "  'HELM: Question answering - Benchmarking',\n",
       "  'HELM: Question answering, Summarization - Benchmarking',\n",
       "  'HELM: Reasoning - Benchmarking',\n",
       "  'HELM: Robustness to contrast sets - Benchmarking',\n",
       "  'HELM: Summarization - Benchmarking',\n",
       "  'HELM: Toxicity - Benchmarking',\n",
       "  'HELM: Toxicity detection - Benchmarking',\n",
       "  'Hugging Face: Conversational - Benchmarking',\n",
       "  'Hugging Face: Fill-mask, Text generation - Benchmarking',\n",
       "  'Hugging Face: Question answering - Benchmarking',\n",
       "  'Hugging Face: Summarization - Benchmarking',\n",
       "  'Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking',\n",
       "  'In-The-Wild Jailbreak Prompts on LLMs - Attack',\n",
       "  'JailbreakingLLMs - Attack\\r\\n',\n",
       "  'LLM Privacy - Attack',\n",
       "  'LegalBench - Benchmarking (with algorithmic and human scoring)',\n",
       "  'MASSIVE - Benchmarking',\n",
       "  'MIMIR - Benchmarking',\n",
       "  'MT-bench - Benchmarking (with human and model scoring)',\n",
       "  'Mark My Words - Benchmark',\n",
       "  \"Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)\",\n",
       "  'TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack',\n",
       "  'The Self-Perception and Political Biases of ChatGPT - Benchmarking',\n",
       "  'Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking',\n",
       "  'detect-pretrain-code - Attack/Benchmarking']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in intermediate_dict:\n",
    "    intermediate_dict[key] = []\n",
    "    for tc in gai_risk_to_tc_map[key]:\n",
    "        intermediate_dict[key] += list(data_dict['low_risk_measure_by_tc'][tc].values)\n",
    "    intermediate_dict[key] = sorted(list(set(intermediate_dict[key])))[1:]\n",
    "    print(key, len(intermediate_dict[key]))\n",
    "    \n",
    "intermediate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d60639db-a00b-4e76-906a-e8ed886dd9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Risk</th>\n",
       "      <th>CBRN Information</th>\n",
       "      <th>Confabulation</th>\n",
       "      <th>Dangerous or Violent Recommendations</th>\n",
       "      <th>Data Privacy</th>\n",
       "      <th>Environmental</th>\n",
       "      <th>Human-AI Configuration</th>\n",
       "      <th>Information Integrity</th>\n",
       "      <th>Information Security</th>\n",
       "      <th>Intellectual Property</th>\n",
       "      <th>Obscene, Degrading, and/or Abusive Content</th>\n",
       "      <th>Toxicity, Bias, and Homogenization</th>\n",
       "      <th>Value Chain and Component Integration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "      <td>An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "      <td>BELEBELE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "      <td>Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "      <td>Big-bench: Context Free Question Answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "      <td>Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "      <td>Big-bench: Convince Me (specific task) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "      <td>Big-bench: Creativity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "      <td>Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "      <td>Big-bench: Low-resource language, Non-English, Translation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "      <td>Big-bench: Morphology, Grammar, Syntax - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "      <td>Big-bench: Out-of-Distribution Robustness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "      <td>Big-bench: Paraphrase - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "      <td>Big-bench: Self-Awareness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "      <td>Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "      <td>Big-bench: Sufficient information - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "      <td>Big-bench: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "      <td>Big-bench: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "      <td>Big-bench: Truthfulness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "      <td>BloombergGPT - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "      <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "      <td>DecodingTrust: Fairness - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "      <td>DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "      <td>DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "      <td>DecodingTrust: Stereotype Bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "      <td>DecodingTrust: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "      <td>Eval Gauntlet\\r\\nReading comprehension - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: Language Understanding - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "      <td>Eval Gauntlet: World Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "      <td>Evaluation Harness: BLiMP - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "      <td>Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CoQA, ARC - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "      <td>Evaluation Harness: CrowS-Pairs - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ETHICS - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "      <td>Evaluation Harness: GLUE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "      <td>Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "      <td>Evaluation Harness: MuTual - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "      <td>Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "      <td>Evaluation Harness: ToxiGen - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Background Knowledge - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Metacognition - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "      <td>FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "      <td>Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "      <td>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "      <td>HELM: Bias - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "      <td>HELM: Knowledge - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "      <td>HELM: Language (Twitter AAE) - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "      <td>HELM: Language - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "      <td>HELM: Memorization and copyright - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "      <td>HELM: Miscellaneous text classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "      <td>HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "      <td>HELM: Question answering, Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "      <td>HELM: Reasoning - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "      <td>HELM: Robustness to contrast sets - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "      <td>HELM: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "      <td>HELM: Toxicity - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "      <td>HELM: Toxicity detection - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "      <td>Hugging Face: Conversational - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "      <td>Hugging Face: Fill-mask, Text generation - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "      <td>Hugging Face: Question answering - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "      <td>Hugging Face: Summarization - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "      <td>Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td></td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "      <td>In-The-Wild Jailbreak Prompts on LLMs - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td></td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td></td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "      <td>JailbreakingLLMs - Attack\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td></td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "      <td>LLM Privacy - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "      <td>LegalBench - Benchmarking (with algorithmic and human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "      <td>MASSIVE - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "      <td>MIMIR - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td></td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "      <td>MT-bench - Benchmarking (with human and model scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td></td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "      <td>Mark My Words - Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td></td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "      <td>Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td></td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "      <td>TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "      <td>The Self-Perception and Political Biases of ChatGPT - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "      <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td></td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "      <td>detect-pretrain-code - Attack/Benchmarking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Risk                                                                                     CBRN Information  \\\n",
       "0                                                   Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "1                                                                Big-bench: Self-Awareness - Benchmarking   \n",
       "2                                                                  Big-bench: Truthfulness - Benchmarking   \n",
       "3                           Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "4     DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "5                      HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "6                                                  HELM: Question answering, Summarization - Benchmarking   \n",
       "7                                                          In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "8                                                                           JailbreakingLLMs - Attack\\r\\n   \n",
       "9                                                                                    LLM Privacy - Attack   \n",
       "10                                                                                   MIMIR - Benchmarking   \n",
       "11                                                                              Mark My Words - Benchmark   \n",
       "12                                 TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "13                                                             detect-pretrain-code - Attack/Benchmarking   \n",
       "14                                                                                                          \n",
       "15                                                                                                          \n",
       "16                                                                                                          \n",
       "17                                                                                                          \n",
       "18                                                                                                          \n",
       "19                                                                                                          \n",
       "20                                                                                                          \n",
       "21                                                                                                          \n",
       "22                                                                                                          \n",
       "23                                                                                                          \n",
       "24                                                                                                          \n",
       "25                                                                                                          \n",
       "26                                                                                                          \n",
       "27                                                                                                          \n",
       "28                                                                                                          \n",
       "29                                                                                                          \n",
       "30                                                                                                          \n",
       "31                                                                                                          \n",
       "32                                                                                                          \n",
       "33                                                                                                          \n",
       "34                                                                                                          \n",
       "35                                                                                                          \n",
       "36                                                                                                          \n",
       "37                                                                                                          \n",
       "38                                                                                                          \n",
       "39                                                                                                          \n",
       "40                                                                                                          \n",
       "41                                                                                                          \n",
       "42                                                                                                          \n",
       "43                                                                                                          \n",
       "44                                                                                                          \n",
       "45                                                                                                          \n",
       "46                                                                                                          \n",
       "47                                                                                                          \n",
       "48                                                                                                          \n",
       "49                                                                                                          \n",
       "50                                                                                                          \n",
       "51                                                                                                          \n",
       "52                                                                                                          \n",
       "53                                                                                                          \n",
       "54                                                                                                          \n",
       "55                                                                                                          \n",
       "56                                                                                                          \n",
       "57                                                                                                          \n",
       "58                                                                                                          \n",
       "59                                                                                                          \n",
       "60                                                                                                          \n",
       "61                                                                                                          \n",
       "62                                                                                                          \n",
       "63                                                                                                          \n",
       "64                                                                                                          \n",
       "65                                                                                                          \n",
       "66                                                                                                          \n",
       "67                                                                                                          \n",
       "68                                                                                                          \n",
       "69                                                                                                          \n",
       "70                                                                                                          \n",
       "71                                                                                                          \n",
       "72                                                                                                          \n",
       "73                                                                                                          \n",
       "74                                                                                                          \n",
       "75                                                                                                          \n",
       "76                                                                                                          \n",
       "77                                                                                                          \n",
       "78                                                                                                          \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                              Confabulation  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                       Dangerous or Violent Recommendations  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                               Data Privacy  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                              Environmental  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "7                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "8                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "9                                                                                                                                                                                                                         Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "10                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "11                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "12                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "13                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "14                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "15                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "16                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "17                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "18                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "19                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "20                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "21                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "22                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "23                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "24                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "25                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "26                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "27                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "28                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "29                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "30                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "31                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "32                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "33                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "34                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "35                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "36                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "37                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "38                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "39                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "40                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "41                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "42                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "43                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "44                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "45                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "46                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "47                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "48                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "49                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "50                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "51                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "52                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "53                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "54                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "55                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "56                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "57                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "58                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "59                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "60                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "61                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "62                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "63                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "64                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "65                                                                                                                                                                                                                                                                                \n",
       "66                                                                                                                                                                                                                                                                                \n",
       "67                                                                                                                                                                                                                                                                                \n",
       "68                                                                                                                                                                                                                                                                                \n",
       "69                                                                                                                                                                                                                                                                                \n",
       "70                                                                                                                                                                                                                                                                                \n",
       "71                                                                                                                                                                                                                                                                                \n",
       "72                                                                                                                                                                                                                                                                                \n",
       "73                                                                                                                                                                                                                                                                                \n",
       "74                                                                                                                                                                                                                                                                                \n",
       "75                                                                                                                                                                                                                                                                                \n",
       "76                                                                                                                                                                                                                                                                                \n",
       "77                                                                                                                                                                                                                                                                                \n",
       "78                                                                                                                                                                                                                                                                                \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                     Human-AI Configuration  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                      Information Integrity  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                       Information Security  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                                      Intellectual Property  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                         Obscene, Degrading, and/or Abusive Content  \\\n",
       "0                                                                                                                               BELEBELE - Benchmarking   \n",
       "1                                                                                                 Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "2                                                                             Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "3                                                                                                              Big-bench: Self-Awareness - Benchmarking   \n",
       "4                                                                       Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "5                                                                                                                    Big-bench: Toxicity - Benchmarking   \n",
       "6                                                                                                                Big-bench: Truthfulness - Benchmarking   \n",
       "7                                                                                                                DecodingTrust: Fairness - Benchmarking   \n",
       "8                                                                                                         DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "9                                                                                                                DecodingTrust: Toxicity - Benchmarking   \n",
       "10                                                                                                 Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "11                                                          Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "12                                                                                                       Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "13                                                                                                           Evaluation Harness: ToxiGen - Benchmarking   \n",
       "14                                                              Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "15    From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "16                                                                                                                            HELM: Bias - Benchmarking   \n",
       "17                                                                                                          HELM: Language (Twitter AAE) - Benchmarking   \n",
       "18                                                                                                      HELM: Memorization and copyright - Benchmarking   \n",
       "19                                                                   HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "20                                                                                               HELM: Question answering, Summarization - Benchmarking   \n",
       "21                                                                                                                        HELM: Toxicity - Benchmarking   \n",
       "22                                                                                                              HELM: Toxicity detection - Benchmarking   \n",
       "23                                                                                                                                 LLM Privacy - Attack   \n",
       "24                                                                                                                               MASSIVE - Benchmarking   \n",
       "25                                                                                                                                 MIMIR - Benchmarking   \n",
       "26                                                                                                                            Mark My Words - Benchmark   \n",
       "27                                                                                   The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "28                                                 Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "29                                                                                                                                                        \n",
       "30                                                                                                                                                        \n",
       "31                                                                                                                                                        \n",
       "32                                                                                                                                                        \n",
       "33                                                                                                                                                        \n",
       "34                                                                                                                                                        \n",
       "35                                                                                                                                                        \n",
       "36                                                                                                                                                        \n",
       "37                                                                                                                                                        \n",
       "38                                                                                                                                                        \n",
       "39                                                                                                                                                        \n",
       "40                                                                                                                                                        \n",
       "41                                                                                                                                                        \n",
       "42                                                                                                                                                        \n",
       "43                                                                                                                                                        \n",
       "44                                                                                                                                                        \n",
       "45                                                                                                                                                        \n",
       "46                                                                                                                                                        \n",
       "47                                                                                                                                                        \n",
       "48                                                                                                                                                        \n",
       "49                                                                                                                                                        \n",
       "50                                                                                                                                                        \n",
       "51                                                                                                                                                        \n",
       "52                                                                                                                                                        \n",
       "53                                                                                                                                                        \n",
       "54                                                                                                                                                        \n",
       "55                                                                                                                                                        \n",
       "56                                                                                                                                                        \n",
       "57                                                                                                                                                        \n",
       "58                                                                                                                                                        \n",
       "59                                                                                                                                                        \n",
       "60                                                                                                                                                        \n",
       "61                                                                                                                                                        \n",
       "62                                                                                                                                                        \n",
       "63                                                                                                                                                        \n",
       "64                                                                                                                                                        \n",
       "65                                                                                                                                                        \n",
       "66                                                                                                                                                        \n",
       "67                                                                                                                                                        \n",
       "68                                                                                                                                                        \n",
       "69                                                                                                                                                        \n",
       "70                                                                                                                                                        \n",
       "71                                                                                                                                                        \n",
       "72                                                                                                                                                        \n",
       "73                                                                                                                                                        \n",
       "74                                                                                                                                                        \n",
       "75                                                                                                                                                        \n",
       "76                                                                                                                                                        \n",
       "77                                                                                                                                                        \n",
       "78                                                                                                                                                        \n",
       "\n",
       "Risk                                                                                                                                                                                                                                         Toxicity, Bias, and Homogenization  \\\n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)   \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking   \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking   \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking   \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking   \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking   \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking   \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking   \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking   \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking   \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking   \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking   \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking   \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking   \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking   \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking   \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking   \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking   \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking   \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking   \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack   \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking   \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)   \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking   \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking   \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking   \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking   \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking   \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking   \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking   \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking   \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking   \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking   \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking   \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking   \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking   \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking   \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking   \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking   \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking   \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)   \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)   \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)   \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)   \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking   \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking   \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking   \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking   \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking   \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking   \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking   \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking   \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)   \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking   \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking   \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking   \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking   \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking   \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking   \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking   \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking   \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking   \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking   \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking   \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking   \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack   \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n   \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack   \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)   \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking   \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking   \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)   \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark   \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)   \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack   \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking   \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking   \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking   \n",
       "\n",
       "Risk                                                                                                                                                                                                                                      Value Chain and Component Integration  \n",
       "0                                                                                                                                                 An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B)  \n",
       "1                                                                                                                                                                                                                                                       BELEBELE - Benchmarking  \n",
       "2     Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking  \n",
       "3                                                                                                               Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking  \n",
       "4                                                                                                                                                                                                                     Big-bench: Context Free Question Answering - Benchmarking  \n",
       "5                                                                                                                                                                           Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking  \n",
       "6                                                                                                                                                                                                                         Big-bench: Convince Me (specific task) - Benchmarking  \n",
       "7                                                                                                                                                                                                                                          Big-bench: Creativity - Benchmarking  \n",
       "8                                                                                                                                                                                                  Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking  \n",
       "9                                                                                                                                                                                                     Big-bench: Low-resource language, Non-English, Translation - Benchmarking  \n",
       "10                                                                                                                                                                                                                        Big-bench: Morphology, Grammar, Syntax - Benchmarking  \n",
       "11                                                                                                                                                                                                                     Big-bench: Out-of-Distribution Robustness - Benchmarking  \n",
       "12                                                                                                                                                                                                                                         Big-bench: Paraphrase - Benchmarking  \n",
       "13                                                                                                                                                                                                                                     Big-bench: Self-Awareness - Benchmarking  \n",
       "14                                                                                                                                                                                              Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking  \n",
       "15                                                                                                                                                                                                                             Big-bench: Sufficient information - Benchmarking  \n",
       "16                                                                                                                                                                                                                                      Big-bench: Summarization - Benchmarking  \n",
       "17                                                                                                                                                                                                                                           Big-bench: Toxicity - Benchmarking  \n",
       "18                                                                                                                                                                                                                                       Big-bench: Truthfulness - Benchmarking  \n",
       "19                                                                                                                                                                                                                                                  BloombergGPT - Benchmarking  \n",
       "20                                                                                                                                                                                                Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack  \n",
       "21                                                                                                                                                                          DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "22                                                                                                                                                                                                                                       DecodingTrust: Fairness - Benchmarking  \n",
       "23                                                                                                                                                                                                     DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation)  \n",
       "24                                                                                                                                          DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking  \n",
       "25                                                                                                                                                                                                                                DecodingTrust: Stereotype Bias - Benchmarking  \n",
       "26                                                                                                                                                                                                                                       DecodingTrust: Toxicity - Benchmarking  \n",
       "27                                                                                                                                                                                                                        Eval Gauntlet\\r\\nReading comprehension - Benchmarking  \n",
       "28                                                                                                                                                                                   Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking  \n",
       "29                                                                                                                                                                                                                         Eval Gauntlet: Language Understanding - Benchmarking  \n",
       "30                                                                                                                                                                                                                                Eval Gauntlet: World Knowledge - Benchmarking  \n",
       "31                                                                                                                                                                                                                                     Evaluation Harness: BLiMP - Benchmarking  \n",
       "32                                                                                                                                                                                  Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \\r\\nTranslation - Benchmarking  \n",
       "33                                                                                                                                                                                                                                 Evaluation Harness: CoQA, ARC - Benchmarking  \n",
       "34                                                                                                                                                                                                                               Evaluation Harness: CrowS-Pairs - Benchmarking  \n",
       "35                                                                                                                                                                                                                                    Evaluation Harness: ETHICS - Benchmarking  \n",
       "36                                                                                                                                                                                                                                      Evaluation Harness: GLUE - Benchmarking  \n",
       "37                                                                                                                                               Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking  \n",
       "38                                                                                                                                                                                                                                    Evaluation Harness: MuTual - Benchmarking  \n",
       "39                Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking  \n",
       "40                                                                                                                                                                                                                                   Evaluation Harness: ToxiGen - Benchmarking  \n",
       "41                                                                                                                                                                                                    FLASK: Background Knowledge - Benchmarking (with human and model scoring)  \n",
       "42                                                                                                                                FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring)  \n",
       "43                                                                                                                                                                                                           FLASK: Metacognition - Benchmarking (with human and model scoring)  \n",
       "44                                                                                                                                                                                FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring)  \n",
       "45                                                                                                                                                                                      Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking  \n",
       "46                                                                                                                            From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking  \n",
       "47                                                                                                                                                                                                                                                    HELM: Bias - Benchmarking  \n",
       "48                                                                                                                                                                                                                                               HELM: Knowledge - Benchmarking  \n",
       "49                                                                                                                                                                                                                                  HELM: Language (Twitter AAE) - Benchmarking  \n",
       "50                                                                                                                                                                                                                                                HELM: Language - Benchmarking  \n",
       "51                                                                                                                                                                                                                              HELM: Memorization and copyright - Benchmarking  \n",
       "52                                                                                                                                                                                                                       HELM: Miscellaneous text classification - Benchmarking  \n",
       "53                                                                                                                                                                                           HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring)  \n",
       "54                                                                                                                                                                                                                                      HELM: Question answering - Benchmarking  \n",
       "55                                                                                                                                                                                                                       HELM: Question answering, Summarization - Benchmarking  \n",
       "56                                                                                                                                                                                                                                               HELM: Reasoning - Benchmarking  \n",
       "57                                                                                                                                                                                                                             HELM: Robustness to contrast sets - Benchmarking  \n",
       "58                                                                                                                                                                                                                                           HELM: Summarization - Benchmarking  \n",
       "59                                                                                                                                                                                                                                                HELM: Toxicity - Benchmarking  \n",
       "60                                                                                                                                                                                                                                      HELM: Toxicity detection - Benchmarking  \n",
       "61                                                                                                                                                                                                                                  Hugging Face: Conversational - Benchmarking  \n",
       "62                                                                                                                                                                                                                      Hugging Face: Fill-mask, Text generation - Benchmarking  \n",
       "63                                                                                                                                                                                                                              Hugging Face: Question answering - Benchmarking  \n",
       "64                                                                                                                                                                                                                                   Hugging Face: Summarization - Benchmarking  \n",
       "65                                                                                                                                                                             Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking  \n",
       "66                                                                                                                                                                                                                               In-The-Wild Jailbreak Prompts on LLMs - Attack  \n",
       "67                                                                                                                                                                                                                                                JailbreakingLLMs - Attack\\r\\n  \n",
       "68                                                                                                                                                                                                                                                         LLM Privacy - Attack  \n",
       "69                                                                                                                                                                                                               LegalBench - Benchmarking (with algorithmic and human scoring)  \n",
       "70                                                                                                                                                                                                                                                       MASSIVE - Benchmarking  \n",
       "71                                                                                                                                                                                                                                                         MIMIR - Benchmarking  \n",
       "72                                                                                                                                                                                                                       MT-bench - Benchmarking (with human and model scoring)  \n",
       "73                                                                                                                                                                                                                                                    Mark My Words - Benchmark  \n",
       "74                                                                                                                                                                                Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring)  \n",
       "75                                                                                                                                                                                                       TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack  \n",
       "76                                                                                                                                                                                                           The Self-Perception and Political Biases of ChatGPT - Benchmarking  \n",
       "77                                                                                                                                                                         Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking  \n",
       "78                                                                                                                                                                                                                                   detect-pretrain-code - Attack/Benchmarking  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_risk_measure_by_gai_risk_frame = pd.DataFrame(columns=data_dict['gai_risk']['Risk'], index=range(0, 79))\n",
    "\n",
    "for j, risk in enumerate(low_risk_measure_by_gai_risk_frame.columns):\n",
    "    for i, eval_ in enumerate(list(intermediate_dict[risk])):     \n",
    "        low_risk_measure_by_gai_risk_frame.iloc[i, j] = eval_\n",
    "    if low_risk_measure_by_gai_risk_frame[risk].isnull().any().any():\n",
    "            low_risk_measure_by_gai_risk_frame[risk] = low_risk_measure_by_gai_risk_frame[risk].replace(np.nan, '')\n",
    "\n",
    "low_risk_measure_by_gai_risk_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b829c538-186b-41c5-9d86-dd6b9260d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "CBRN Information \\\\\n",
      "\\midrule\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Confabulation \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Dangerous or Violent Recommendations \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Data Privacy \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Environmental \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Human-AI Configuration \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Information Integrity \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Information Security \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Intellectual Property \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Obscene, Degrading, and/or Abusive Content \\\\\n",
      "\\midrule\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      " \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Toxicity, Bias, and Homogenization \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{l}\n",
      "\\toprule\n",
      "Value Chain and Component Integration \\\\\n",
      "\\midrule\n",
      "An Evaluation on Large Language Model Outputs: Discourse and Memorization - Benchmarking (with human scoring, see Appendix B) \\\\\n",
      "BELEBELE - Benchmarking \\\\\n",
      "Big-bench: Algorithms, Logical reasoning, Implicit reasoning, Mathematics, Arithmetic, Algebra, Mathematical proof, Fallacy, Negation, Computer code, Probabilistic reasoning, Social reasoning, Analogical reasoning, Multi-step, Understanding the World - Benchmarking \\\\\n",
      "Big-bench: Analytic entailment (specific task), Formal fallacies and syllogisms with negation (specific task), Entailed polarity (specific task) - Benchmarking \\\\\n",
      "Big-bench: Context Free Question Answering - Benchmarking \\\\\n",
      "Big-bench: Contextual question answering, Reading comprehension, Question generation - Benchmarking \\\\\n",
      "Big-bench: Convince Me (specific task) - Benchmarking \\\\\n",
      "Big-bench: Creativity - Benchmarking \\\\\n",
      "Big-bench: Emotional understanding, Intent recognition, Humor - Benchmarking \\\\\n",
      "Big-bench: Low-resource language, Non-English, Translation - Benchmarking \\\\\n",
      "Big-bench: Morphology, Grammar, Syntax - Benchmarking \\\\\n",
      "Big-bench: Out-of-Distribution Robustness - Benchmarking \\\\\n",
      "Big-bench: Paraphrase - Benchmarking \\\\\n",
      "Big-bench: Self-Awareness - Benchmarking \\\\\n",
      "Big-bench: Social bias, Racial bias, Gender bias, Religious bias - Benchmarking \\\\\n",
      "Big-bench: Sufficient information - Benchmarking \\\\\n",
      "Big-bench: Summarization - Benchmarking \\\\\n",
      "Big-bench: Toxicity - Benchmarking \\\\\n",
      "Big-bench: Truthfulness - Benchmarking \\\\\n",
      "BloombergGPT - Benchmarking \\\\\n",
      "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation - Attack \\\\\n",
      "DecodingTrust: Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Fairness - Benchmarking \\\\\n",
      "DecodingTrust: Machine Ethics - Benchmarking (Machine Ethics Evaluation) \\\\\n",
      "DecodingTrust: Out-of-Distribution Robustness, Adversarial Robustness, Robustness Against Adversarial Demonstrations - Benchmarking \\\\\n",
      "DecodingTrust: Stereotype Bias - Benchmarking \\\\\n",
      "DecodingTrust: Toxicity - Benchmarking \\\\\n",
      "Eval Gauntlet\n",
      "Reading comprehension - Benchmarking \\\\\n",
      "Eval Gauntlet: Commonsense reasoning, Symbolic problem solving, Programming - Benchmarking \\\\\n",
      "Eval Gauntlet: Language Understanding - Benchmarking \\\\\n",
      "Eval Gauntlet: World Knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: BLiMP - Benchmarking \\\\\n",
      "Evaluation Harness: C-Eval (Chinese evaluation suite), MGSM, \n",
      "Translation - Benchmarking \\\\\n",
      "Evaluation Harness: CoQA, ARC - Benchmarking \\\\\n",
      "Evaluation Harness: CrowS-Pairs - Benchmarking \\\\\n",
      "Evaluation Harness: ETHICS - Benchmarking \\\\\n",
      "Evaluation Harness: GLUE - Benchmarking \\\\\n",
      "Evaluation Harness: HellaSwag, OpenBookQA - General commonsense knowledge, TruthfulQA - Factuality of knowledge - Benchmarking \\\\\n",
      "Evaluation Harness: MuTual - Benchmarking \\\\\n",
      "Evaluation Harness: PIQA, PROST - Physical reasoning, MC-TACO - Temporal reasoning, MathQA - Mathematical reasoning, LogiQA - Logical reasoning, SAT Analogy Questions - Similarity of semantic relations, DROP, MuTual â€“ Multi-step reasoning - Benchmarking \\\\\n",
      "Evaluation Harness: ToxiGen - Benchmarking \\\\\n",
      "FLASK: Background Knowledge - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Logical correctness, Logical robustness, Logical efficiency, Comprehension, Completeness - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Metacognition - Benchmarking (with human and model scoring) \\\\\n",
      "FLASK: Readability, Conciseness, Insightfulness - Benchmarking (with human and model scoring) \\\\\n",
      "Finding New Biases in Language Models with a Holistic Descriptor Dataset - Benchmarking \\\\\n",
      "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models - Benchmarking \\\\\n",
      "HELM: Bias - Benchmarking \\\\\n",
      "HELM: Knowledge - Benchmarking \\\\\n",
      "HELM: Language (Twitter AAE) - Benchmarking \\\\\n",
      "HELM: Language - Benchmarking \\\\\n",
      "HELM: Memorization and copyright - Benchmarking \\\\\n",
      "HELM: Miscellaneous text classification - Benchmarking \\\\\n",
      "HELM: Narrative Reiteration, Narrative Wedging - Benchmarking (with human scoring) \\\\\n",
      "HELM: Question answering - Benchmarking \\\\\n",
      "HELM: Question answering, Summarization - Benchmarking \\\\\n",
      "HELM: Reasoning - Benchmarking \\\\\n",
      "HELM: Robustness to contrast sets - Benchmarking \\\\\n",
      "HELM: Summarization - Benchmarking \\\\\n",
      "HELM: Toxicity - Benchmarking \\\\\n",
      "HELM: Toxicity detection - Benchmarking \\\\\n",
      "Hugging Face: Conversational - Benchmarking \\\\\n",
      "Hugging Face: Fill-mask, Text generation - Benchmarking \\\\\n",
      "Hugging Face: Question answering - Benchmarking \\\\\n",
      "Hugging Face: Summarization - Benchmarking \\\\\n",
      "Hugging Face: Text classification, Token classification, Zero-shot classification - Benchmarking \\\\\n",
      "In-The-Wild Jailbreak Prompts on LLMs - Attack \\\\\n",
      "JailbreakingLLMs - Attack\n",
      " \\\\\n",
      "LLM Privacy - Attack \\\\\n",
      "LegalBench - Benchmarking (with algorithmic and human scoring) \\\\\n",
      "MASSIVE - Benchmarking \\\\\n",
      "MIMIR - Benchmarking \\\\\n",
      "MT-bench - Benchmarking (with human and model scoring) \\\\\n",
      "Mark My Words - Benchmark \\\\\n",
      "Putting GPT-3's Creativity to the (Alternative Uses) Test - Benchmarking (with human scoring) \\\\\n",
      "TAP: A Query-Efficient Method for Jailbreaking Black-Box LLMs - Attack \\\\\n",
      "The Self-Perception and Political Biases of ChatGPT - Benchmarking \\\\\n",
      "Towards Measuring the Representation of Subjective Global Opinions in Language Models - Benchmarking \\\\\n",
      "detect-pretrain-code - Attack/Benchmarking \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for risk in low_risk_measure_by_gai_risk_frame.columns:\n",
    "    print(low_risk_measure_by_gai_risk_frame[risk].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d3da1-7818-4347-818c-9c654f28b94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
